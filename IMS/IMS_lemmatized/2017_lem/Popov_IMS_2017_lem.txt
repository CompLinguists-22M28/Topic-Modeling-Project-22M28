"чёрный лебеди": извлечение редкий событие из текст а.м. попов, ю.в. адаскин infoqubes санкт-петербургский государственный университет москва, санкт-петербург задача поиск аномалия в текст становиться всё более востребовать в область анализ клиентский опыта. это связать как с наработать за последний год знание о отзыв клиентов, так и с сдвиг интерес от статистически значимый к статистически незначимый в другой научный дисциплинах. статистический анализ частотный и легко прогнозировать причина обращение клиент в служба контроль качество и клиентский негативный опыт — задача не новый и до известный степень решенная. противоположный же она задача — поиск и идентификация редкий и нечастотный аспект негативный опыт клиент — задача весьма новый и не иметь на сегодняшний день стандартный подход к решению. извлечение аномалия (anomaly detection) существовать как самостоятельный задача в многий областях, где применяться машинный обучение и анализ большой объём данных, а также при создание различный система мониторинга. в ход наш исследование мы разрабатывать инструментарий для анализ такой случай для коммерческий проекта, а сам аномалия получить метафорический название «чёрный лебеди» вслед за известный работа экономист н. талеб «чёрный лебедь. под знак непредсказуемости». обычно считается, что именно этот книга и предложить в она терминология привлечь внимание к исследование редкий непредсказуемый событий, иметь значительный последствия. наш метод в многое опираться на идею, предложить в [1], где для извлечение «черных лебедей» использоваться семантический информация о словах, получить путём анализ «независимых» корпусов, например wordnet. кроме того, хороший результат давать использование информация о семантический близость слово (см. [2], [3]). другой подход основать на представление документ из корпус как многомерный таблица совместный встречаемость слов, для анализ который применяться нейронный сеть ([4]) или svm-классификатор ([5]). стандартный методика поиск главный проблем, или негативный тем, обычно представлять себя частотный анализ различный лингвистический объектов, извлекать из исследовать текстов: слово и они нормальный форм, словосочетание (n-грамм), разрывный конструкций, синтаксический связь или фрагмент синтаксический структура и т.д. этот метод достаточно эффективен, так как для сортировать частотный список хорошо работать принцип парето: небольшой выборка из верхний часть частотный список покрывать большинство случаев, однако при это оставаться «хвост» нечастотный элемент огромный длины. очевидно, что поиск аномалия в инвертировать частотный список неэффективный из-за большой количество «мусора», ведь далеко не всё низкочастотный объект являться аномальными. поэтому при разработка наш инструмент мы решить вместо частотный критерий использовать критерий лексический сочетаемости. в некоторый смысл наш подход напоминать метод извлечение коллокация из корпус текстов: если пара слово встречаться вместе часто, а по отдельность этот слово встречаться редко, то вес такой сочетание как устойчивый быть высоким. при это в наш эксперимент мы использовать не биграммы, а пара синтаксически связанный слов, получить в ход синтаксический анализа. метрика аномальность синтаксический связь вычисляться по формуле: wr = fr / (fs + ft), где fr — это частота совместный встречаемость два лемма в рамка синтаксический отношения, а fs и ft — соответственно самостоятельный частота лемма-вершина и леммы-зависимого. чем выше значение wr, тем выше аномальность сочетание этот два лемм. такой образом, на подготовительный этап мы провести полный лингвистический анализ весь корпус текстов, накопить за несколько год сотрудничество с один из наш заказчиков, и построить по он два частотный списка: список единичный лемма и список синтаксически связанный пара лемм. в статистика не включаться слово и сочетания, который встретиться в корпус только один раз. этап собственно поиск аномалия в наш понимание состоять из два частей: поиск аномальный документ из предложить выборки; поиск ключевой слов, маркировать аномальность, в этот документах. в качество критерий аномальность документ мы взять собственный метрику, производный от вес синтаксический связей, входящий в этот документ. наш гипотеза, поздний подтвердить экспериментально, состоять в следующем: аномальный документ иметь некоторый количество связь с высокий весом, в то время как для нормальный документ распределение вес связь более равномерное. это позволить мы взять за вес документ отношение сумма вес n наиболее весомый связь к n наименее весомым, соответственно, чем выше это соотношение — тем выше аномальность документа. формально вес документ можно описать так: wd = ∑a1…an / ∑am-n…am где ai — это показатель аномальность связи, m — это число связь в документе, а n — размер выборка связь для вычисление вес (в наш эксперимент n = 10). теперь у мы есть весь необходимый информация для анализ корпус на предмет выявление аномалий. для решение задача поиск «черных лебедей», или аномальный документ в коллекция текст мы использовать сортировка документ по критерий аномальности. для решение задача маркирование аномальный слово в отобрать подмножество текст мы подсвечивать в документ слова, входящий в наиболее весомый синтаксический связи. первичный верификация методика происходить на основа экспертный разметка корпус документов, результат подтвердить работоспособность предложить алгоритма: тексты, разметить вручную как аномальные, получить значительно более высокий значение показатель аномальности.