РЕАЛИЗАЦИЯ АЛГОРИТМА АВТОМАТИЧЕСКОГО ИСПРАВЛЕНИЯ
ОПЕЧАТОК В РУССКОЯЗЫЧНЫХ ТЕКСТАХ
В.В. Сампетова, В.О. Луканина, А.Д. Москвина, О.А. Митрофанова
Санкт-Петербургский государственный университет
Санкт-Петербург
Введение
Сегодня создание эффективного и доступного инструмента автоматического исправления опечаток в
текстах на русском языке является особенно актуальной задачей в связи с распространением цифровых
технологий и расширением сферы применения устройств с текстовым вводом–выводом, использующих
методы и алгоритмы автоматической обработки естественного языка.
Решение задачи автоматического исправления опечаток имеет огромное практическое значение, это
важно как в повседневной жизни при обмене сообщениями в социальных сетях, при переписке в
мессенджерах, так и в научных исследованиях, проводимых на материале так называемых “грязных” текстов
из сети Интернет. Корпусы текстов из новостных электронных СМИ и из социальных сетей, как правило,
имеют большой размер (с одной стороны, есть возможность собрать такие данные, с другой – цели и методы,
связанные с такими текстами, требуют существенных объемов данных, в частности, для машинного
обучения). Такие тексты рассматриваются как “грязные” из-за изобилия опечаток, дублетов, нетекстовых
элементов. Для обработки на морфосинтаксическом и семантическом уровнях они должны пройти
графематический анализ. Одна из проблем, решаемых на данном этапе, связана именно с выявлением и
исправлением опечаток, что в дальнейшем повышает точность токенизации и морфологической аннотации
текстов. Актуальность исследований в области автоматического исправления опечаток в русскоязычных
текстах подтверждается в обсуждениях материалов соревнования SpellRuEval−2016, прошедшего в рамках
Международной конференции по компьютерной лингвистике и интеллектуальным технологиям
“Диалог−2016” [1].
Опечатки являются наиболее простым видом орфографических ошибок. Большинство предлагаемых
решений связано с вычислением вероятности замен в зависимости от расположения букв на клавиатуре, как
например, в работе [2]. Наряду с этим, были созданы фонетические алгоритмы для исправления
орфографических ошибок. Примером подобного алгоритма является, в частности, SOUNDEX [3], в котором
кодируется произношение слов, и для каждого кода формируется набор символов-омофонов, из которых
подбирается правильный вариант на замену.
Большинство созданных алгоритмов исправляет каждое отдельно взятое слово. Несмотря на то, что
сейчас все большее внимание уделяется разработке подходов с контекстно-зависимым исправлением, при
изучении литературы было обнаружено, что и без применения подобных подходов процент правильных
замен достаточно высок. Согласно данным, обсуждаемым в работе [4], “для большей части словарных
ошибок (74%) исправления в контексте и без контекста совпадают”, соответственно, в нашем исследовании
на данном этапе он использоваться не будет.
Таким образом, в настоящее время существует множество различных подходов к решению данной
проблемы, но большинство алгоритмов создается для английского языка. И в связи с различием английского
и русского языков на всех лингвистических уровнях, исправление опечаток в словах на русском языке
требует разработки собственного подхода и более подходящего алгоритма.
В основу нашего подхода легли результаты исследования 1964 г. Ф.Дж. Дамерау об определении и
исправлении ошибок правописания. Им было выявлено, что 80% всех ошибочно напечатанных слов имели
лишь одну ошибочную позицию из следующих: вставка, удаление, замена или перестановка [5].
Так как алгоритм должен не только определять ошибочное слово, но и исправлять его, были
использованы техники исправления ошибок в отдельных словах, особое внимание было уделено
типографическим ошибкам [6], как наиболее частым.
Структура статьи такова: раздел 2 посвящен описанию использованных нами методов; в разделе 3
приведены лингвистические данные материалы для обучения алгоритма и проверки его работы; в разделе 4
приведены результаты проведенной работы, выводы и планы для будущей работы.
Методы
Предположение о том, что опечатки возникают из-за некорректного ввода с клавиатуры электронных
устройств, послужило основой для выбора методов данной работы. Также вспомогательным материалом для
создания кода послужила программа, свободно размещенная на Github (https://github.com/Machyne/Spelling).
С помощью языка Python была создана симметричная карта для всех пар букв с соответствующими им

весами замен. Были учтены горизонтальная, вертикальная и диагональная близость букв на клавиатуре, а
также схожие по звучанию буквы и адаптированы для русского языка. Например, для буквы “а” были
подобраны следующие пары: по горизонтали − “а” − “п”; “а” − “в”; по вертикали − “а” − “м”; “а”
− “с”; по диагонали − “а” − “у”; “а” − “е”. Также были приняты во внимание ошибки, допускаемые
независимо от расположения клавиш на клавиатуре, для “а” это такие пары как, “а” − “о”: “молоко” −
“малако”; “а” − “я”: “чаща” − “чящя”; “а” − “и”: “маленький” − “маленькай”. К данной матрице был
применен алгоритм расчета расстояния Левенштейна. Расстояние Левенштейна, или дистанция
редактирования − это минимальное количество операций (вставка, удаление или замена), которые
направлены для преобразования одной строки в другую кратчайшим способом. При применении данного
алгоритма образуется матрица дистанций, в данном случае была получена матрица значений близости пар
букв. В этой матрице дистанций указаны числовые значения, которые зависят от того, в каком расположении
от рассматриваемой буквы находятся все остальные, так, если буква “в” находится рядом с “а”, в ячейке
матрицы сохраняется вес 1,5. Вес чисел подобран вручную и служит для обозначения только вероятности
замены букв при вводе, чем ближе буква находится по отношению к нужной, значит, тем меньше
вероятность записать ее правильно, поэтому меньшее число означает большую вероятность ошибки (см.
табл. 2). В случае, если буквы в матрице совпадают, по умолчанию ставится 0, для дополнительного
значения, которое не зависит от близости клавиш − 2. Также для ввода лишней буквы или пропуска выбрано
значение 3, оно не представлено в матрице, поскольку реализуется непосредственно в формуле
Левенштейна.


Следующим шагом было применение спеллчекера. Каждое предложение во входном тексте посредством
библиотеки NLTK [7] токенизируется, т.е. разбиваются на отдельно значимые единицы (слова, символы,
пробелы) для последующей компьютерной обработки. Второй список токенов затем создается только из
слов. Этот список инвертируется, и каждое слово проверяется. Каждому слову приписывается вероятность,
которая представляет собой среднее вероятностей триграммы этого слова: самого слова, допустим, B,
предыдущего от него − A и следующего за ним – C: например, для слова “пошел” триграмма будет выглядеть
следующим образом: яA пошелB гулятьC. Эти вероятности триграмм рассчитаны при помощи взвешенной
линейной интерполяции (P) вероятностей униграмм, биграмм и триграмм, посчитаны как коэффициент
счетов.
P(w |w -1...w ) = a P (w ) + a P (w |w ) + … +a P (w |w -1...w -1), где 0<=a <=1 и ∑ i a =1 , где P −
статистическая функция для i-грамм.


Каждому слову приписывается список лучших кандидатов на замену. Для того чтобы получить лучшего
кандидата, сначала определяется набор кандидатов. Этот набор включает в себя каждое слово, которое
может следовать за двумя предыдущими, и каждое слово, которое начинается с буквы равной, “схожей”, или
горизонтально смежной по отношению к первой букве слова с предполагаемой опечаткой. Этот вес,
приписанный к любому кандидату, − это линейная комбинация расстояния для замены в ошибочном слове и
вероятность, которую оно получит, если произойдет замена. Список лучших замен – это 10 первых
кандидатов по счету. Наконец, если у слова крайне низкая вероятность, и программа не установила лучшего
кандидата, то оно будет отмечено как “неправильное” и пользователю предложено самому исправить
отмеченное слово.
Исследовательский материал
Тестирование инструменты было проведено на материале публицистических статей информационного
веб-портала Медуза [8]. Для получения тестовых результатов из тренировочных текстов были произвольно
2

выбраны слова и заменены на ошибочные. В дальнейшем для получения экспериментальных результатов
будет использован материал из корпуса ЖЖ ГИКРЯ (30 млн с/у), который был предоставлен для конкурса
MorphRuEval−2017, прошедшего в рамках Международной конференции по компьютерной лингвистике и
интеллектуальным технологиям «Диалог−2017» [9]. Поскольку в данном корпусе собраны уже
исправленные тексты из социальных сетей (LiveJournal), то обучение будет проходить именно на них, а
проверка работоспособности данного алгоритма будет проверена на “грязных” текстах, которые позволят
выяснить реальные возможности и эффективность программы.
Результаты
Первым шагом работы алгоритма был запуск тренировочного файла через консоль/терминал. После
этого в командной строке запускается файл с ошибкой и пропускается через уже обученную программу.
Алгоритм находит ошибочное слово, предлагает 10 вариантов замены и возможность ввести слово самим
пользователем, принимает правильный вариант замены. Можно заключить, что процесс обучения проходит
правильно, так как он определяет слова с опечатками, и на первом месте предлагает нужный вариант на
замену (см. табл. 2).


Заключение
Данная работа посвящена реализации алгоритма для автоматического исправления опечаток. Опечатки
составляют большую часть ошибок в печатном тексте, и являются наиболее простыми для правильного
автоматического исправления, так как не зависят от контекста, в котором использовано слово.
На нынешнем этапе проведения экспериментов работа алгоритма считается удовлетворительной и
эффективной: алгоритм находит слово с ошибкой и предлагает правильного кандидата на замену, также
предлагает ввести собственный вариант, если ни один из кандидатов не подходит. Тем не менее,
используемое программное обеспечение требует доработки, что предполагает его адаптацию в Python 3.5,
совершенствование кода, добавление ряда орфографических правил, которые смогут улучшить работу
алгоритма и устранить недостатки вероятностного подхода. На следующем этапе эксперименты будут
проведены на более обширном и разнообразном материале.
Исследование поддержано грантом РФФИ № 16-06-00529 «Разработка лингвистического комплекса для
автоматического семантического анализа русскоязычных корпусов текстов с применением статистических
методов» (2016–2018 гг.).