Сравнение NLP-моделей на задаче суммаризации академических текстов на русском языке
Д.В. Мельничук, А.В. Носкина
Саратовский национальный исследовательский государственный университет 
имени Н.Г. Чернышевского
1. Введение
Основной целью данной работы является ответ на вопрос: «Какая из NLP-моделей суммаризации (Natural Language Processing, NLP – Обработка текстов на естественном языке) наиболее оптимально работает в контексте академической литературы на русском языке»? Под суммаризацией текста понимается процесс автоматического сокращения объема исходного текста путем извлечения наиболее важных и существенных идей, фактов и информации, а также представления в форме краткого и сводного текста, который сохраняет основные аспекты исходного материала.
Для сравнения эффективности разных типов предобученных (pre-trained) NLP-моделей использовался набор статей из открытой научной электронной библиотеки CyberLeninka, из части массива доступных данных, были использованы тексты (text) научных статей и соответствующие аннотации (annotation) их авторов на русском языке. Всего 825 статей, среди которых область наук и тип журнала брались в случайном порядке.
2. Модели и данные
Для исследования были выделены наиболее популярные, по версии ресурса HuggingFace Hub, открытые NLP-модели суммаризации текста, обученные на одном и том же корпусе новостных текстов на русском языке (Gazeta) [5].
Языковая модель GPT-3 (Generative Pre-trained Transformer) использует механизмы трансформеров для анализа контекста и генерации последовательностей слов, учитывая вероятность каждого следующего слова на основе предыдущих слов в тексте. Модель также способна выполнить различные задачи, такие, как ответы на вопросы, перевод текстов на другие языки и создание текстовых статей. В нашем исследовании была использована GPT-3 модель, обученная под задачу суммаризации, под кодовым названием модели на ресурсе HuggingFace Hub: RuGPT3MediumSumGazeta [6].
Модель T5 (Text-to-Text Transfer Transformer) также использует архитектуру трансформеров и обучается на задачах преобразования текста в текст. На вход подается задание и исходный текст, а затем генерируется выходной текст, решающий поставленную задачу. Модель обучается на широком спектре задач, включая машинный перевод, генерацию текста, ответы на вопросы, классификацию текста и многое другое. Для нашего исследования была использована базовая модель RuT5Base, обученная на новостных текстах на русском языке (Gazeta) под задачу суммаризации: RuT5SumGazeta [7].
Модель mBART (multilingual Bidirectional and Auto-Regressive Transformer) использует технологию мультиязычного перевода, обученную на большом количестве текстов на разных языках. Каждый язык представлен в виде уникального кода, и модель может работать с несколькими языками одновременно. При обучении модели mBART используется подход обучения с подкреплением, который позволяет модели улучшать свой перевод по мере того, как она получает обратную связь. Архитектура трансформеров позволяет данной модели учитывать контекст и зависимости между словами в предложении. Аналогичным образом использована базовая модель mBART, обученная на новостных текстах на русском языке (Gazeta) под задачу суммаризации: MBARTRuSumGazeta [8].
3. Метрики
Для оценки и сравнения языковых моделей используются два подхода.
Первый подход - это внешняя оценка (External evaluation), при которой оценивание модели происходит за счёт решения с её помощью задачи, на которую она рассчитана, и дальнейший анализ итоговых показателей потерь/точности, а также является лучшим подходом к оцениванию моделей, так как это единственный способ реально оценить, как разные модели справляются с интересующей нас задачей. Однако реализация данного подхода может потребовать больших вычислительных мощностей, его применение может оказаться медленным, так как для этого нужно обучение всей анализируемой системы (BLEU, ROUGE – это внешняя оценка). 
Второй же подход - это внутренняя оценка (Internal evaluation), которая производит оценку самих языковых моделей, без учёта конкретных задач, для решения которых их планируется использовать; она является не столь информативной для понимания качества работы модели на конкретной задаче, как внешняя, но, если необходимо провести итоговую оценку модели, то данный подход может быть весьма эффективным для быстрого сравнения моделей (Perplexity – это внутренняя оценка).
В данной работе были использованы метрики: BLEU, семейство метрик ROUGE и Perplexity.
Метрика BLEU (Bilingual Evaluation Understudy) – это алгоритм оценки качества машинной генерации текста (в том числе перевода), основанный на сравнении выходных текстов, т.е. сгенерированных (predictions) с известными, эталонными (references) текстами. Сам подход заключается в сравнении двух вариантов текста, по совпадению слов и их расположению, также это называют схожести n-грамм (последовательности n слов). В итоге получается количественная оценка соответствия между результатом работы NLP-модели и результатом работы человека: чем ближе машинная генерация к исходному тексту человека, тем он лучше - такова основная идея BLEU. Метрика BLEU включает корректировки весов, такие, как фактор бонуса на основе bi-граммов и сглаживание на основе ковариационной матрицы предложений, чтобы справиться с некоторыми из проблем данного подхода [3].
Пусть C – множество слов сгенерированного текста, R – множество слов эталонного текста, соответственно c_i и r_i – это i -е слова этих множеств (списков). Пусть n - максимальная длина n-грамм, которые мы рассматриваем. Тогда BLEU оценивает качество сгенерированного текста с путем вычисления взвешенного гармонического среднего точности n-грамм:
ROUGE (Recall-Oriented Understudy for Gisting Evaluation) – это набор показателей (семейство метрик) для оценки автоматического суммирования текстов (в том числе машинного перевода), основанный на сравнении n-грамм сгенерированного (predictions) текста с n-граммами эталонных (references) текстов. Основная идея метрики ROUGE заключается в сравнении двух текстов и подсчете базовых единиц (n-грамм, т. е. последовательностей слов и количества пар слов). В результате получаем количественную оценку работы NLP-модели, которая показывает, насколько сгенерированный текст совпадает с текстом, составленным человеком (экспертом). В отличие от BLEU, ROUGE использует как полноту (recall), так и точность (precision) для сравнения сгенерированных текстов с эталонными текстами, составленными человеком [2].
В ROUGE-1 сравниваются единицы (слова) между сгенерированным и эталонным текстами. В ROUGE-2 сравниваются последовательности из двух слов, взятых из сгенерированного и эталонного текста. В ряде источников ROUGE-1 и ROUGE-2 могут обозначаться общей записью ROUGE-N. ROUGE-L, в свою очередь, не сравнивает n-граммы, а обрабатывает тексты и ищет самую длинную последовательность (LCS), которая является общей для двух текстов, а затем измеряет ее длину.
Пусть S – сгенерированный текст, G – эталонный текст, соответственно s_i и g_i - это i-е слова в S и G. ROUGE-N оценивает качество генерации из S путем вычисления точности совпадения слов в S с G, подсчитывает количество совпадающих (co-occurrences) n-грамм (для ROUGE-1 это одно слово, для ROUGE-2 это последовательность из двух слов), найденных как в выходных данных модели, так и в эталоне, а затем делит это число на общее количество n-грамм в S:
Метрика Perplexity в языковых моделях используется для оценки того, насколько хорошо модель может предсказать следующее слово в тексте. Для хорошей NLP-модели метрика Perplexity будет давать высокие вероятности синтаксически корректным предложениям, а предложениям некорректным (или очень редко встречающимся) – низкие вероятности. При условии, что набор данных состоит из корректных предложений, лучшей моделью будет та, которая назначит наивысшую вероятность этому тестовому набору, что означает то, что модель обладает хорошим пониманием того, как устроен язык [4].
4. Методология
Методология исследования выглядит следующим образом: изначально выделяются данные, включающие в себя авторскую аннотацию и три варианта автоматически сгенерированных для каждого отдельно взятого текста статьи NLP-моделями суммаризаций, а затем проводится сравнение на близость сгенерированных текстов с исходным текстом авторской аннотации. 
В результате для каждой исходной статьи формируется оценка по пяти метрикам для трех NLP-моделей. Далее, имея данные результаты, находится среднее по каждому показателю, что и является итоговой оценкой эффективности работы данных NLP-моделей на задаче суммаризации. 
5. Результаты и выводы
В результате, на задаче суммаризации академических текстов на русском языке, наилучшим образом проявила себя модель T5, которая показала наибольшую эффективность на основе статистических метрик. Данный результат может быть обусловлен тем, что модель T5 обеспечивает лучшую производительность и точность на задаче суммаризации текста, благодаря своей более общей и гибкой архитектуре, а также улучшенным параметрам и настройкам, в отличие от mBART и GPT-3. 
В дальнейших исследованиях планируется расширение проверочного набора статей, сравнение большего числа моделей, разделение проверочного датасета на области наук и сравнение результатов дискретно по научным областям. 