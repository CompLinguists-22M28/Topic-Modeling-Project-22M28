Методы машинного обучения применительно к задаче 
выделения глагольных и атрибутивных коллокаций
М.В. Хохлова, Е.В. Еникеева
Санкт-Петербургский государственный университет
Введение
Исследование сочетаемости не теряет своей актуальности на протяжении последних десятилетий. Определение лексических конструкций и их последующий анализ весьма важны для современных прикладных задач: создание словарей тональности, расширение поисковых запросов, автоматический перевод и др. Описание развития статистических методов, применяемых к нахождению в корпусах текстов словосочетаний разных типов, приведено в работе [1]. Для автоматического извлечения сочетаний используются разные статистические методы, в том числе ставшие широко распространенными в последние годы методы машинного обучения, которые нашли применение при решении лингвистических задач в связи с появлением как больших текстовых данных, так и технических возможностей.
В статье рассматриваются результаты экспериментов по автоматическому выделению и последующей оценке атрибутивных и глагольных словосочетаний при помощи моделей машинного обучения на русскоязычных коллекциях текстов большого объема.
1. Обзор методов
Традиционные методы извлечения лексических конструкций используют лексико-грамматические шаблоны для поиска возможных примеров, которые затем ранжируются в соответствии с одной из статистических мер, отражающих степень связи между двумя признаками. Для вычисления совместной встречаемости необходимо учитывать совместную частоту встречаемости признаков (фиксированного и переменного компонента конструкции) в корпусе, что не дает возможности извлекать реализации конструкций, не встречающиеся в корпусе.
Современные методы, основанные на машинном обучении, решают, в частности, эту проблему, используя методики сглаживания данных. Популярные в последнее время модели языка, основанные на нейронных сетях (англ. a neural probabilistic language model), позволяют оценить вероятность не встреченной в корпусе последовательности: этот подход основан на получении информации о более широком контексте слова, чем в классических статистических моделях, для предсказания следующего слова в цепочке.
Векторное представление слов (англ. word embeddings) стало еще одним методом машинного обучения, который нашел применение при автоматическом извлечении словосочетаний. Векторные представления слов используются для представления связей между словами: синонимии, гипонимии и семантических аналогий [2]. Группа моделей, которые представляют слова, носит название word2vec и описана в работе [там же]. В отличие от традиционной дистрибутивной семантики word2vec основан на алгоритме машинного обучения, согласно которому слово может быть предсказано на основе его контекста (архитектура непрерывного мешка слов, англ. continuous bag-of-words) или наоборот, контекст в зависимости от слова (архитектура skip gram).
Исследование лингвистической композициональности [3; 4] показало, что дистрибутивные представления могут быть использованы для моделирования отношений в рамках словосочетаний довольно простым образом. Например, атрибутивное словосочетание признается значимым, если его векторное представление (вычисленное как сумма составляющих векторов) близка к вектору опорного слова. Таким образом, словосочетания могут быть отранжированы по их приемлемости согласно функции близости (например, косинусного сходства, англ. cosine similarity):
sim(noun, adj) = cos(noun + adj, noun)
Но для нормализованных векторов функции sim(noun, adj) и cos(noun, adj) будут одинаково монотонны, то есть для оценки степени приемлемости словосочетания достаточно оценить близость входящих в него слов. Такой подход мы будем использовать в качестве базовой оценки (англ. baseline).
Несмотря на то, что были достигнуты значительные результаты, методы редко используются для кодирования синтагматических отношений. Авторы [5] используют word2vec для предсказания коллокатов из словаря Macmillan Collocations Dictionary. Некоторые эксперименты по русскоязычным коллокациям были описаны в [6; 7]. Так, в работе [6] был протестирован данный метод на материале русского языка, показавший точность до 0,9.
2. Методология исследования
Как было отмечено выше, для работы с алгоритмами машинного обучения необходим довольно большой объем текстовых данных, а также нужны примеры эталонных данных, относительно которых будут оцениваться результаты.
В ходе работы над проектом коллокации нами извлекались в два этапа из Интернет-корпуса Aranea Russicum Maximum [8]. В качестве эталонных коллокаций, т.е. золотого стандарта, использовались словосочетания ряда словарей (толкового и специализированных): Толковый словарь русского языка в 4-х томах (МАС) [9], Словарь коллокаций [10], Словарь глагольной сочетаемости непредметных имен русского языка [11], Словарь русской идиоматики [12] и Словарь устойчивых глагольно-именных словосочетаний русского языка [13]. На их основе был создан список эталонных конструкций.
На первой стадии нами были разработаны правила, которые описывали русскоязычные словосочетания и были применены к извлечению данных. Были рассмотрены следующие синтаксические модели: 1) прилагательное + существительное; 2) глагол + существительное; 3) существительное + глагол. Второй этап включал использование модели word2vec для оценки извлеченных словосочетаний. Векторные представления были получены с помощью данного инструмента, обученного на текстах Национального корпуса русского языка (НКРЯ [14]). Список образцов лексических конструкций собран по данным синтаксически размеченного подкорпуса НКРЯ – СинТагРус.
Нами были использованы простые векторные операции к векторными представлениям слова, чтобы найти семантические аналогии, как это описано в работе [15]. Результирующий вектор «queen – woman + man» (слова представлены векторами) схож с векторным представлением слова «king».
Мы используем этот подход, чтобы смоделировать синтагматические отношения, а именно, найти наиболее близкий коллокат для данного слова и тип коллокации [6]. Набор коллокаций C, собранный на материале небольшого сбалансированного корпуса разных жанров, используется как пример аналогии (как queen – woman в примере выше).
Приемлемость коллокации w1+w2, где w1 является заглавным словом, вычисляется следующим образом:
 
Всего были рассмотрены 3 метода оценки коллокаций на основе машинного обучения и векторных представлений текстов:
—	базовый метод (англ. baseline), основанный на сходстве векторов слов, составляющих коллокацию; 
—	метод аналогии, основанный на предположении, что разность векторов слов, составляющих одну коллокацию, должна быть близка к разности векторов в эталонных коллокациях; 
—	метод линейного преобразования: из векторных представлений формируются пространства главных слов словосочетания и коллокатов. Затем на основании обучающей выборки строится матрица линейного преобразования между этими пространствами; для новых коллокаций оценивается близость произведения матрицы преобразования на вектор ключевого слова к вектору коллоката.
3. Результаты
В табл. 1 приведены данные по количеству биграмм после фильтрации, соответствующих использованным морфосинтаксическим-шаблонам, а также примеры наиболее устойчивых сочетаний в лемматизированном виде (то есть слова записаны в начальной форме).
Для оценки использовались стандартные меры – точность (precision), полнота (recall), F-мера (F-mean). Последняя является средним гармоническим двух предыдущих. Поскольку результатом автоматического выделения коллокаций является список словосочетаний, ранжированный по оценке одного из предложенных алгоритмов M, были проведены эксперименты по подбору порога значений M, который позволял бы отделить коллокации от свободных словосочетаний. Стоит подчеркнуть, что оценивались именно точность и полнота полученного списка коллокаций, а не их ранжирование (поскольку золотой стандарт не был отранжирован).
Оценка результатов была произведена относительно собранного золотого стандарта. Было сделано сравнение двух списков словосочетаний, соответствующих определенному шаблону (например, прилагательное + существительное), – золотой стандарт G и результат автоматического выделения A. В таблице 2 представлены результаты экспериментов.
Ниже приведены примеры словосочетаний, которые были извлечены автоматическими методами (изначально они являются лемматизированными), но при этом отсутствуют в рассмотренных словарях: «кандидатская диссертация», «фондовая биржа», «злокачественная опухоль», «смертная казнь»; «оканчивать школу», «надевать платье», «снимать кинофильм», «уничтожать противника»; «осадки выпадают», «войско двинулось», «избиратель голосует», «Бог смилостивился», «рассудок помутился».
4. Обсуждение
Следует отметить, что значения точности не показательны, поскольку, во-первых, используемые словари сочетаемости составлялись на основе других корпусов и ограничены по своему объему, во-вторых, среди данных золотого стандарта содержатся фразеологизмы и термины, которые имеют низкие частоты, а в-третьих, не учитываются правильно выделенные коллокации, отсутствующие в словарях. По критерию полноты, в отличие от экспериментов по предсказанию значений лексических функций [6], методы 2 и 3 показывают не такие высокие результаты по сравнению с baseline. Возможно, это связано с тем, что отношения внутри коллокаций (пусть и одного синтаксического типа) менее регулярны.
Заключение
Впоследствии будут рассмотрены иные источники, а также будут извлечены словосочетания в рамках других синтаксических моделей. Дополнительно планируется сопоставить полученные результаты с данными экспериментов по извлечению словосочетаний при помощи статистических мер ассоциации.
Статья подготовлена в рамках работы по гранту Президента Российской Федерации для государственной поддержки молодых российских ученых № МК-2513.2018.6 «Исследование методов автоматического извлечения лексических конструкций на основе машинного обучения».
