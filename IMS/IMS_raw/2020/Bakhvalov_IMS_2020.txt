Разработка и реализация методов генерации правил 
для автоматической проверки правописания
П.Я. Бахвалов
Университет ИТМО
Введение
Автоматическая проверка правописания – это задача автоматического обнаружения и исправления грамматических, стилистических и орфографических ошибок в тексте. Актуальность этой задачи определяется тем, что людям свойственно делать ошибки правописания. Далеко не все хорошо знакомы с правилами грамматики языка или только приступают к их изучению. Помимо этого, существуют болезни, такие как дисграфия, при которых человеку особенно тяжело правильно писать. Задача осложняется за счет таких факторов как неформализованная грамматика, свободный порядок слов в предложении, зависимость слов от контекста, постоянные изменения в языке, диалекты, сленг, жаргон, омонимы и др.
Подходы к задаче автоматической проверки правописания можно разделить на три типа: основанные на правилах, основанные на методах машинного обучения и гибридные. Подход, основанный на правилах, считается устаревшим, т.к. правила обычно пишут квалифицированные лингвисты, имеет низкую полноту, но более высокую точность. С другой стороны, подходы на основе машинного обучения показывают высокое покрытие ошибок и сейчас являются state-of-the-art. Гибридные же подходы обычно выигрывают, т.к. всегда можно взять за основу подход на машинном обучении и добавить в него правила, для которых взятый подход плохо работает, тем самым увеличивая точность и полноту.
Несмотря на значительный прогресс в методах решения задачи проверки правописания, основанных на машинном обучении, у подхода, основанного на правилах, остается ряд существенных преимуществ:
—	интерпретируемость правил;
—	возможность редактирования набора правил;
—	низкая потребность в вычислительных ресурсах.
Квалифицированный специалист всегда может посмотреть на каждое конкретное правило и определить, соответствует ли оно правилу в естественном языке. Пользователь системы также сможет получить информацию о правиле, которое он нарушил. 
В системе, основанной на правилах, новые правила можно просто добавить в общий список. То же самое относится к ошибкам в правилах: лингвист сможет исправить правило, добавить исключение или выключить его в такой системе, в то время как в системе, основанной на машинном обучении, ошибка будет возникать снова и необходимо полностью переобучать модель.
Системы, основанные на машинном обучении, гораздо более требовательны к ресурсам, поэтому их невозможно использовать локально на устройствах пользователя. К сожалению, не все пользователи имеют доступ к безлимитному и бесперебойному интернету для взаимодействия с такими системами на выделенных серверах; особые случаи связаны с работой с конфиденциальной информацией, где недопустима отправка данных по сети, в особенности на сторонний сервер. Модель, которая заняла первое место на последнем соревновании по автоматическому исправлению ошибок (BEA-2019) [1], занимает 3.7Гб в сжатом виде, в то время как системы, основанные на правилах, занимают всего десятки мегабайт.
Основной сложностью систем, основанной на правилах, является то, что правила пишутся вручную квалифицированными лингвистами. Когда количество правил достигает тысяч, становится проблематично отслеживать полноту правил и проверять, покрывает ли каждое правило все случаи, в том числе исключения. Решением этих проблем может быть автоматическая генерация правил. Существует ряд работ в этом направлении. Авторы [2] предлагают подход, основанный на имитации иммунной системы человека, когда ошибки рассматриваются как патогены, а правила как лимфоциты. Существуют подходы, основанные на расстоянии редактирования на уровне слов [3]. Однако ни один из подходов не работает хорошо на всех типах ошибок. Большинство описанных в литературе реализаций инструментов недоступно для публичного использования. К тому же сложно сравнивать результаты разных методов, т.к. в каждой работе используется свой набор данных. 
На практике системы проверки правописания на основе правил активно используются и развиваются в настоящее время. Одной из таких систем является LanguageTool, для которого с 2003 года команда лингвистов пишет правила на основе присылаемых им ошибок [4]. Данная система является бесплатной и имеет открытый исходный код, а также подходит для автономного использования на локальном компьютере, поэтому набор правил данной системы был взят за основу настоящей работы и сгенерированные правила в итоге будут адаптированы под неё.
В настоящей работе был выбран английский язык, т.к. он является основным языком научных коммуникаций и технической документации.
Авторы обзора систем автоматической проверки грамматики предлагают следующую классификацию типов ошибок [5]: грамматика, пунктуация, структура предложения, орфография, семантика.
В данной работе будут рассматриваться только ошибки грамматики, пунктуации и ошибки в структуре предложения. Исправление орфографических ошибок хорошо разработанная область, точность исправления таких ошибок уже достигает 91% [6]. Категория семантики, с другой стороны, слишком сложна, и предложения с такими ошибками имеют правильную грамматическую структуру. Для определения такой ошибки часто требуется учитывать контекст нескольких предложений, а иногда и целого текста. Как исключение, есть редкая подкатегория ошибок, когда люди используют похожее по смыслу слово, но которое недопустимо в контексте данного словосочетания. Например: «This is a durable (strong) drink». Но такие ошибки исправляются по словарю подобных словосочетаний и не входят в тематику настоящей работы. Поэтому задача генерации правил для ошибок из категории семантики не имеет смысла.
1. Обзор существующих методов
Исследования по автоматическому исправлению правописания ведутся с 60-х годов 20 века. Уже тогда были попытки решения задачи с использованием обычного словаря, n-граммной модели, и языковых моделей [7]. Новый импульс исследованиям придали открытые соревнования, которые проводятся с 2011 года. Первым в своем роде соревнованием является HOO-2011, который предоставил первый общий набор данных для работы с ошибками правописания в открытый доступ, а также общую систему оценивания работ [8]. Потом был CoNLL-2014 [9] и последний из таких контестов BEA-2019, набор данных и система проверки которого были использованы в настоящей работе для проверки качества полученных результатов [1].
В 2011 году подход на основе байесовских классификаторов для исправления ошибок в артиклях, предлогах и некорректном выборе слов продемонстрировал превосходство над подходами, основанными на правилах [10]. В 2014 году лучшие результаты показали подходы, основанные на статистическом машинном переводе [11]. Далее стали появляться подходы, основанные на использовании языковых моделей [12; 13]. Позже для решения задачи исправления ошибок в тексте стали использоваться более сложные модели машинного обучения, например, трансформеры [14; 15]. Лучшим подходом на текущий момент на данных BEA-2019 является система, представляющая собой ансамбль моделей других участников соревнования [16].
2. Подготовка данных для экспериментов
2.1. Исходные данные
В качестве набора данных были использованы данные, предоставленные соревнованием BEA-2019, оно состоит из двух частей. Первая часть – данные с онлайн-платформы Write & Improve [17], которая предлагает помощь при написании текстов людям, для которых английский язык не является родным. Вторая часть набора данных – The LOCNESS corpus [18] – сборник сочинений студентов-носителей английского языка, размеченный сотрудниками Write & Improve. 
Общий объем данных – примерно 40000 предложений с ошибками и их исправлениями, сделанными одновременно и носителями, и не носителями языка [1]. Пример из набора данных представлен на рисунке 1.
2.2. Разделение по категориям ошибок
На начальном этапе необходимо было разделить данные на группы по типам ошибок. Для данной задачи был использован инструмент для аннотации грамматических ошибок ERRANT [19]. В результате все ошибки были поделены по типам и по категориям (см. табл. 1 и 2).
Всего получилось 54 категории ошибок – прямое произведение типов и категорий за исключением тех, что не встречаются в исходных данных. Примеры из каждой категории были сгруппированы, причем ошибки, не относящиеся к данной категории, были исправлены, чтобы избежать их влияния на генерацию правил. Тем самым мы предполагаем, что в категории ошибок нет ни одной другой ошибки кроме как из данной категории.
2.3. Выделение признаков
После разделения данных по ошибкам из слов и предложения были выделены признаки (см. табл. 3) в качестве основы для генерации правил. 
Пример размеченного предложения представлен на рисунке 2. 
Для выделения признаков был использована библиотека StanfordNLP [20].
3. Генерация правил
3.1. Представление правила
Процесс проверки правописания выглядит следующим образом: предложение разбивается на токены, в нем выделяются признаки, описанные выше, после чего начинается итерирование по каждому токену. Токен подается в правило, как базовый элемент, от которого правило будет отталкиваться. В общем случае правило состоит из двух компонент: детектор и редактор. Детектор – это набор условий, которые должны быть выполнены в случае наличия ошибки. Условие – это тип признака вместе с ожидаемым его значением, либо логическая комбинация из других условий, а также смещение относительно проверяемого токена. Редактор – это набор изменений, которые должны произойти с предложением. Изменение состоит из:
—	смещения относительно проверяемого токена;
—	списка новых токенов, которые необходимо вставить перед изменяемым токеном;
—	списка модификаций над изменяемым токеном (изменение регистра слова, изменение части речи слова, полная замена);
—	флага о необходимости удаления токена.
Данный формат удобен для генерации правил в отличие от формата, принятого в LanguageTool. В нем правило выглядит похожим образом, поэтому трансляция из одного формата в другой при необходимости не вызывает сложностей (см. рис. 3). 
<pattern> – это сгенерированное правило, внутри которого <token> является словом, на которое надо реагировать, в параметрах которого можно задавать признаки. <marker> отвечает за то, что будет являться ошибкой для пользователя и что нужно подчеркнуть. <example> – пример, берется из тестового набора данных. <suggestion> – это то, на что надо исправить <marker>, генерируется из редактора, внутри него можно использовать преобразования частей речи и обращаться к токенам в правиле по индексам. В итоге лингвисту остается только проверить осмысленность правила и придумать ему описание, хотя его тоже можно сгенерировать автоматически.
3.2. Transformation-Based Learning
В качестве метода генерации правил было использовано обучение на основе трансформаций [21]. Этот метод использовался, например, для генерации правил морфологической разметки. Процесс генерации правил выглядит следующим образом. На вход подается набор данных с ошибками и их исправлениями. По каждой ошибке генерируется правило для её детектирования согласно шаблону из заданного списка. К детектору добавляется редактор, который будет исправлять ошибку, т.е. набор изменений, которые необходимо сделать для исправления. На данном этапе мы получили список правил-кандидатов. 
Теперь будем итерироваться по каждому кандидату и применять его ко всему набору данных, считая метрику качества. На каждой итерации выбираем лучшее правило по заданной метрике и применяем его ко всему набору данных. Повторяем, пока остаются ошибки, или результат не станет удовлетворительным (см. рис. 4).
Этот метод очень хорошо себя показал во многих областях обработки естественного языка [21]. Однако для применения метода к задаче исправления ошибок в тексте есть несколько препятствий:
—	неясно, откуда брать список шаблонов для правил;
—	подсчет метрики каждый раз для каждого правила занимает много времени;
—	метрика плохо подходит для нашей задачи;
—	неясно, какое правило считать лучшим на итерации.
Потенциально правилом может являться любая комбинация из признаков, а количество шаблонов очень быстро растет с ростом количества признаков и токенов. Если пересчитывать метрику каждый раз, то уже на 1000 правилах время выполнения будет достигать нескольких дней на персональном компьютере.
Если правило имеет большое количество одновременно ложных и корректных срабатываний, оно будет более приоритетным, в то время как более-менее продуктивное правило без ложных срабатываний будет выбрано гораздо позже или не выбрано вовсе. Помимо этого, возможна ситуация, когда два менее общих правила, которые будут покрывать то же множество ошибок, будут работать лучше и давать меньше ложных срабатываний. Применение более общего правила на более раннем этапе не позволит генерировать более специфичные правила позже.
Метод был взят за основу и модифицирован с учетом выявленных недостатков.
3.3. Предложенный метод
За основу была взята часть корпуса Гутенберга [22] размером в ~2 млн слов, без грамматических и прочих ошибок. Данные были разбиты на токены, выделены признаки. После чего был построен граф возможных последовательностей признаков длиной в три токена (далее тройка). На основе такой структуры можно быстро находить количество троек по заданному набору условий для трех подряд идущих токенов. Такие же графы были сгенерированы для всего набора данных и для троек, включающих ошибку из набора данных.
Для каждой ошибки в наборе генерируется каждая возможная тройка условий (порядка 250 тысяч на ошибку). Далее, тройки, которые часто (более 1% от всех троек) встречаются в «чистом» графе, а также тройки, которые реагируют на слишком малое количество (менее четырех) ошибок отфильтровываются, после чего выбирается первая тысяча лучших троек. После этого для каждой отфильтрованной тройки на основе графа из набора данных собираются индексы слов, на которые данная тройка будет реагировать, и рассчитывается количество правильных и ложных срабатываний. В итоге остаются только те тройки, которые могут исправить более 3 ошибок и имеют менее 1000 ложных срабатываний.
Для каждой тройки с индексами генерируются условия для четвертого и пятого токена, получая тем самым наборы пятерок. Для каждой пятерки заново рассчитываются правильные и ложные срабатывания: при добавлении новых условий, количество срабатываний может только лишь уменьшится, следовательно, достаточно проверить только те срабатывания правила, которые уже нашлись для базовой тройки. Выбирается лучшая пятерка по заданной метрике для каждой ошибки. Если она имеет точность больше 80% и исправляет не меньше трех ошибок, то такая пятерка считается хорошей и из нее генерируется детектор.
Вместе с детектором единственно возможным образом из изначальной ошибки генерируется редактор. Из детектора и редактора получается правило-кандидат.
После прохождения по всем ошибкам и получения набора кандидатов в правила, дублирующие правила отфильтровываются. Каждый раз выбирается лучшее правило из набора и считается качественным, после чего выбранное правило применяется к набору данных. Затем выполняется перерасчет метрики для всех остальных правил из набора кандидатов и процесс повторяется до тех пор, пока очередное правило не перестанет удовлетворять заданной метрике.
Подробный процесс генерации правил показан на рисунке 5.
В качестве результата работы можно рассмотреть следующие правила (см. рис. 6). В первом примере правило можно сформулировать так: если фраза начинается с предлога in, далее идет любое притяжательное местоимение, за которым следует слово opinion, то после этой фразы следует поставить запятую. Во втором примере мы видим, что если встречается слово many являющееся часть связки с существительным, то существительное, к которому оно относится, должно находиться во множественном числе. Третье правило удаляет определенный артикль the во фразе at the school.
4. Оценка качества
Для оценки качества была использована F-мера, которая представляет собой гармоническое среднее между точностью и полнотой, она стремится к нулю, если точность или полнота стремятся к нулю. Стоит отметить, что при работе с правилами, большее значение придается точности, чем на полноте, потому что важнее правильно исправить ошибку, чем покрыть большее количество ошибок. В таких случаях F-мера может быть модифицирована с учетом необходимого приоритета. В настоящей работе приоритет точности считается в два раза выше приоритета полноты, такую меру принято называть F0.5 (см. рис. 7).
4.1. Результаты генерации правил по категориям
Для части категорий с помощью разработанного метода были сгенерированы правила. Другая часть была исключена. Например, категории опечаток, орфографии, морфологии были исключены т.к. не рассматриваются в данной работе. Категории сокращений, преобразования существительного и глагола, формы прилагательного, были исключены ввиду слишком малого количества ошибок для обучения. Также была исключена категория “остальное”, потому что содержит в себе слишком разнородный набор ошибок. Для остальных категорий результаты представлены в табл. 4.
После генерации правила были проверены на третьей части набора данных, которая не использовалась при генерации и показала следующие результаты (см. табл. 5):
4.2. Качество правил для LanguageTool
LanguageTool был запущен на данных для обучения, после чего правила, у которых было слишком много ложных срабатываний, были выключены. После чего к исходным правилам были добавлены правила, полученные с использованием предложенного подхода (см. табл. 6).
Выводы
С помощью предложенного метода удалось сгенерировать 1238 правил для 36 категорий. Качественнее всего правила получились для категорий пунктуации, формы глагола, согласования глагола с существительным и фразовых глаголах. Для категорий, в основном связанных с заменой частей речи, не удалось сгенерировать качественных и общих правил, т.к. ошибки в них сильнее зависят от контекста предложения, чем от ближайших 5 токенов к ошибке. Результаты на дополнительных данных это подтверждают, категория наречий оказалась впереди лишь по причине слишком малого количества ошибок этого типа в выборке.
Предложенный метод дает определенный прирост в качестве, полученные правила существенно расширили те 3203 правила, которые уже присутствуют в LanguageTool и увеличили количество найденных ошибок. 
В дальнейшем планируется рассмотреть возможность генерации правил не для подряд идущих токенов в предложении, а для токенов, связанных через граф зависимостей, полученный после синтаксического анализа. 
Это позволит сильнее учитывать контекст в рамках одного предложения и генерировать более общие правила для связок существительных с глаголом без учета лишних слов между ними.
