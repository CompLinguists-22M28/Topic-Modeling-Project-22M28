"ЧЕРНЫЕ ЛЕБЕДИ": ИЗВЛЕЧЕНИЕ РЕДКИХ СОБЫТИЙ ИЗ ТЕКСТА
А.М. Попов, Ю.В. Адаскина
InfoQubes
Санкт-Петербургский государственный университет
Москва, Санкт-Петербург
Задача поиска аномалий в текстах становится все более востребованной в области анализа клиентского
опыта. Это связано как с наработанными за последние годы знаниями об отзывах клиентов, так и со сдвигом
интереса от статистически значимого к статистически незначимому в других научных дисциплинах.
Статистический анализ частотных и легко прогнозируемых причин обращения клиентов в службу контроля
качества и клиентского негативного опыта — задача не новая и до известной степени решенная.
Противоположная же ей задача — поиск и идентификация редких и нечастотных аспектов негативного
опыта клиента — задача весьма новая и не имеющая на сегодняшний день стандартных подходов к
решению. Извлечение аномалий (anomaly detection) существует как самостоятельная задача во многих
областях, где применяется машинное обучение и анализ большого объема данных, а также при создании
различных систем мониторинга. В ходе нашего исследования мы разрабатываем инструментарий для
анализа таких случаев для коммерческого проекта, а сами аномалии получили метафорическое название
«черные лебеди» вслед за известной работой экономиста Н. Талеба «Черный лебедь. Под знаком
непредсказуемости». Обычно считается, что именно эта книга и предложенная в ней терминология
привлекла внимание к исследованиям редких непредсказуемых событий, имеющих значительные
последствия.
Наш метод во многом опирается на идею, предложенную в [1], где для извлечения «черных лебедей»
используется семантическая информация о словах, полученная путем анализа «независимых» корпусов,
например WordNet. Кроме того, хорошие результаты дает использование информации о семантической
близости слов (см. [2], [3]). Другие подходы основаны на представлении документов из корпуса как
многомерных таблиц совместной встречаемости слов, для анализа которых применяются нейронные сети
([4]) или SVM-классификатор ([5]).
Стандартные методики поиска главных проблем, или негативных тем, обычно представляют собой
частотный анализ различных лингвистических объектов, извлекаемых из исследуемых текстов: слов и их
нормальных форм, словосочетаний (n-грамм), разрывных конструкций, синтаксических связей или
фрагментов синтаксических структур и т.д. Этот метод достаточно эффективен, так как для сортированного
частотного списка хорошо работает принцип Парето: небольшая выборка из верхней части частотного
списка покрывает большинство случаев, однако при этом остается «хвост» нечастотных элементов
огромной длины. Очевидно, что поиск аномалий в инвертированном частотном списке неэффективен из-за
большого количества «мусора», ведь далеко не все низкочастотные объекты являются аномальными.
Поэтому при разработке нашего инструмента мы решили вместо частотного критерия использовать
критерий лексической сочетаемости. В некотором смысле наш подход напоминает метод извлечения
коллокаций из корпуса текстов: если пара слов встречается вместе часто, а по отдельности эти слова
встречаются редко, то вес такого сочетания как устойчивого будет высоким. При этом в нашем
эксперименте мы использовали не биграммы, а пары синтаксически связанных слов, полученные в ходе
синтаксического анализа. Метрика аномальности синтаксической связи вычисляется по формуле:
wr = fr / (fs + ft),
где fr — это частота совместной встречаемости двух лемм в рамках синтаксического отношения, а fs и ft
— соответственно самостоятельные частоты леммы-вершины и леммы-зависимого. Чем выше значение wr,
тем выше аномальность сочетания этих двух лемм.
Таким образом, на подготовительном этапе мы провели полный лингвистический анализ всего корпуса
текстов, накопленного за несколько лет сотрудничества с одним из наших заказчиков, и построили по нему
два частотных списка: список единичных лемм и список синтаксически связанных пар лемм. В статистику
не включались слова и сочетания, которые встретились в корпусе только один раз.
Этап собственно поиска аномалий в нашем понимании состоит из двух частей:
Поиск аномальных документов из предложенной выборки;
Поиск ключевых слов, маркирующих аномальность, в этих документах.
В качестве критерия аномальности документа мы взяли собственную метрику, производную от весов
синтаксических связей, входящих в этот документ. Наша гипотеза, позднее подтвержденная
экспериментально, состояла в следующем: аномальные документы имеют некоторое количество связей с

высоким весом, в то время как для нормальных документов распределение весов связей более равномерное.
Это позволило нам взять за вес документа отношение суммы весов n наиболее весомых связей к n наименее
весомым, соответственно, чем выше это соотношение — тем выше аномальность документа. Формально вес
документа можно описать так:
wd = ∑a1…an / ∑am-n…am
где ai — это показатель аномальности связи, m — это число связей в документе, а n — размер выборки
связей для вычисления веса (в наших экспериментах n = 10).
Теперь у нас есть вся необходимая информация для анализа корпуса на предмет выявления аномалий.
Для решения задачи поиска «черных лебедей», или аномальных документов в коллекции текстов мы
используем сортировку документов по критерию аномальности. Для решения задачи маркирования
аномальных слов в отобранном подмножестве текстов мы подсвечиваем в документах слова, входящие в
наиболее весомые синтаксические связи. Первичная верификация методики происходила на основе
экспертной разметки корпуса документов, результат подтвердил работоспособность предложенного
алгоритма: тексты, размеченные вручную как аномальные, получили значительно более высокое значение
показателя аномальности.