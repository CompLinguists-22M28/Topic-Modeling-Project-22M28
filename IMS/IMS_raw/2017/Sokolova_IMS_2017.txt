Автоматическое извлечение ключевых слов
и словосочетаний из русскоязычных текстов
с помощью алгоритма KEA
Е.В. Соколова, О.А. Митрофанова
Санкт-Петербургский
государственный
университет,


1. Введение
Наборы назначенных вручную или автоматически выделенных ключевых
слов и словосочетаний текста используются для формирования у пользователя
общего представления о содержании текста. Ключевые слова и словосочетания
чаще всего понимают как структурные единицы текста, содержащие наиболее
важную информацию о содержании текста. Необходимо различать два
основных подхода к решению проблемы автоматизации выделения ключевых
слов и словосочетаний: назначение ключевых слов и словосочетаний (keyphrase
assignment) и их извлечение (keyphrase extraction) [2, 3]. Главное отличие
заключается в том, что первый подход позволяет выделять только те ключевые
слова и словосочетания, которые содержатся в некотором предусмотренном
словаре, а второй подход предполагает выбор ключевой информации
непосредственно из текста.
Существуют различные методы извлечения ключевых слов и
словосочетаний [4, 5]:
статистические
(например,
метрика
TF×IDF),
лингвистические (включающие семантический, синтаксический анализ и т.п.),
методы машинного обучения (наивный байесовский классификатор, метод

опорных векторов и др.), а также гибридные подходы (KEA, TextRank и др.).
Некоторые из них предполагают наличие словарей или фоновых корпусов,
другие не требуют таковых. Стоит также отметить, что каждый отдельный
алгоритм может выделять только ключевые слова, словосочетания или и те, и
другие одновременно. Так как большая часть исследований в этой области
проводится на материале английского языка, то в англоязычной литературе
встречается несколько синонимичных терминов для обозначения ключевых
слов и словосочетаний, например, «key terms», «keyphrases» или «keywords», но
чаще всего используются последние два, причём как для униграмм, так и для nграмм.
В качестве основного направления данной работы мы избрали исследование
одного из алгоритмов автоматического извлечения слов и словосочетаний из
текста, а именно KEA (Keyphrase Extraction Algorithm) [6]. Данный алгоритм
широко известен благодаря своим высоким результатам на материале
английского языка [7, 8, 9], поэтому мы попытались применить его к
русскоязычным текстам и найти оптимальный способ оценки его
эффективности для русского языка.
Таким образом, основная цель нашего исследования — адаптация KEA для
работы с русскоязычным материалом. Для её достижения необходимо
дополнение алгоритма инструментами для работы с русским языком и
подготовка данных для проведения экспериментов. Инструменты включают в
себя модули графематического и морфологического анализа для русского
языка, а также русскоязычный стоп-словарь. Материалом для проверки работы
алгоритма и оценки его эффективности после адаптации служат четыре
корпуса, содержащих тексты по ракетостроению и аэрокосмическим
исследованиям.

2. Внутренняя организация KEA и этапы его работы
KEA был разработан Йеном Виттеном и его коллегами в Новой Зеландии в
1999 г. [1]. Как было отмечено выше, KEA относится к классу гибридных
алгоритмов и включает в себя два этапа:
− машинное обучение с учителем: на вход подаётся обучающая выборка с
выделенными автором или экспертом ключевыми словами и
словосочетаниями; в результате обучения строится модель для
определения ключевых слов и словосочетаний;
− автоматическое извлечение ключевых слов и словосочетаний на основе
построенной модели.

2. 1. Выделение кандидатов в ключевые слова и словосочетания
На каждом из этапов работы выбираются кандидаты в ключевые слова и
словосочетания, для каждого из которых затем вычисляются значения
определённых признаков. Выбор кандидатов, в свою очередь, предполагает три
шага:
1) предварительная обработка подаваемых на вход документов:
– токенизация;
– замена знаков пунктуации и цифр на символы, обозначающие
границы словосочетаний;
– сегментация слов, предполагающих дефисное написание;
– удаление оставшихся небуквенных символов.
2) определение кандидатов с помощью следующего набора правил:
– ограничение максимальной длины словосочетания (как правило,
три слова);
– отбрасывание кандидатов имён собственных;
– отбрасывание кандидатов, содержащих стоп-слово в начале или
конце.
Все последовательности слов, полученных на шаге 1, рассматриваются с
учётом правил, выделенных на шаге 2, в результате чего получается набор
наиболее релевантных на данном этапе обработки кандидатов.
3) выравнивание регистра и стемминг: в оригинальном алгоритме для
английского языка используется стеммер Джули Ловинс, написанный в
1968 г. Первоначальная форма и регистр слов сохраняются для
представления пользователю в том случае, если кандидат действительно
окажется ключевым словом или словосочетанием.
2.2. Вычисление значений основных признаков для каждого кандидата
Для каждого кандидата вычисляются значения двух основных признаков,
используемых в дальнейшем как в обучающей выборке, так и для тестового
набора документов: метрика TF×IDF и расстояние от начала документа до
первого появления рассматриваемого слова или словосочетания в нём.
2.3. Метрика TF×IDF
TF×IDF (term frequency — inverse document frequency) — это статистическая
мера частоты встречаемости слова или словосочетания в конкретном
документе, определяемая в сравнении с частотой его использования в других
документах коллекции или в некотором фоновом корпусе.
Для этой цели KEA создаёт файл, в котором хранит информацию о частоте
встречаемости слова или словосочетания в конкретном обрабатываемом
документе и о количестве документов коллекции, содержащих данную
структурную единицу.
Таким образом, для каждого слова или словосочетания P в документе D
метрика TF×IDF рассчитывается по следующей формуле:

freq(P,D) — количество раз, которое данное слово или словосочетание
встречается в D;
size(D) — количество слов в D;
df(P) — количество документов в некоторой коллекции или фоновом
корпусе, содержащих P.
N — размер коллекции или фонового корпуса.
Второй множитель — это функция правдоподобия, отвечающая за
вероятность появления данного слова или словосочетания в документе
(представлена с отрицанием, так как вероятность меньше единицы). Если
документ не является частью коллекции или фонового корпуса, перёд
вычислением функции параметры df(P) и N увеличиваются на единицу, чтобы
симулировать его появление.
2.4. Расстояние от начала документа до первого появления слова или
словосочетания в нём
Расстояние от начала документа до первого появления слова или
словосочетания в нём является отношением количества слов, предшествующих
данному, и общего количества слов в документе. Результат представлен
значениями 1 или 0 в зависимости от размера части документа до первого
появления данного слова или словосочетания в нём.
2.5. Обучение алгоритма и построение модели прогнозирования
кандидатов в ключевые слова и словосочетания
На данном этапе работы алгоритма используется обучающая выборка с
выделенными автором или экспертом ключевыми словами и словосочетаниями.
Во всех документах определяются кандидаты, для каждого из которых
вычисляются значения описанных выше признаков. Чтобы уменьшить объём
обрабатываемых данных, игнорируются слова с единичной частотой, после чего
каждый кандидат помечается как «ключевой» или «неключевой». Это бинарное
деление является классовым признаком, используемым наивным байесовским
классификатором [10]. Наивный байесовский классификатор представляет
собой классификатор, который определяет вероятность принадлежности
рассматриваемого объекта к одному из заранее определённых классов. При этом
процесс классификации строится на предположении о независимости классов
друг от друга. Таким образом, данный классификатор относит объект X к классу
Ci тогда и только тогда, когда выполняется условие P(Ci|X)>P(Cj|X), где P(Ci|X)
— апостериорная вероятность принадлежности объекта X классу Ci, а P(Cj|X) —
апостериорная вероятность принадлежности объекта X классу Cj. В KEA он
изучает веса, назначенные кандидату, и на их основе часть помечает как
«ключевые», а другую — как «неключевые». Далее он строит модель, которая
предсказывает, к какому из обозначенных классов относится слово или
словосочетание в зависимости от значения вычисленных признаков.

2.6. Извлечение новых ключевых слов и словосочетаний
Чтобы выбрать ключевые слова и словосочетания из нового документа, KEA
определяет кандидатов и вычисляет для каждого из них значения признаков,
после чего применяет модель прогнозирования, построенную на предыдущем
этапе. Она вычисляет полную вероятность того, что каждый кандидат является
ключевым словом или словосочетанием, а затем, после обработки, выбирается
лучший набор ключевых слов и словосочетаний.
Когда классификатор обрабатывает кандидата с признаками t (TF×IDF)
и d (distance), вычисляются две величины:

и аналогичная для P[no], где Y — количество положительных примеров в
обучающей выборке, то есть слова и/или словосочетания, назначенные автором,
а N — количество отрицательных примеров, то есть кандидаты, которые не
являются ключевыми (чтобы избежать нулевой вероятности используется
сглаживание Лапласа, которое Y и N заменяет на Y+1 и N+1).
Полная вероятность того, что кандидат является ключевым словом или
словосочетанием, в свою очередь, вычисляется следующим образом:

Согласно значению этой величины, кандидаты ранжируются и
осуществляются два следующих шага. Во-первых, значение TF×IDF
используется в том случае, если вероятности двух кандидатов равны. Вовторых, из списка удаляются все слова и словосочетания, которые содержатся в
других выражениях, имеющих более высокий ранг. Из полученного
ранжированного списка первые r предоставляются пользователю, где r —
количество запрашиваемых ключевых слов и словосочетаний.

3. Адаптация KEA и оценка его работы на материале русского
языка
3.1. Планирование эксперимента
KEA является универсальным лингвонезависимым алгоритмом, его
программная реализация позволяет использовать его совместно с процессорами
для любого естественного языка. Для проверки эффективности работы данного
алгоритма на материале русскоязычных текстов мы осуществили адаптацию
KEA, совместив его с модулями графематического и морфологического анализа
для русского языка.
KEA реализован на языке программирования Java и поставляется
разработчиками со всеми необходимыми инструментами для работы алгоритма
на материале нескольких языков. Как было отмечено выше, на одном из этапов
своей работы алгоритм подразумевает стемминг. Единственным доступным

инструментом для работы с русскоязычными текстами на Java оказывается
стеммер Портера [11]. Основываясь на особенностях языка, он отсекает лишь
суффиксы и флексии, поэтому на данном этапе нами использовался
морфологический анализатор pymorphy2 [12] для Python. Проводилась
предварительная лемматизация текстов как из обучающей, так и из тестовой
выборок, после чего к обработанным текстам применялся KEA. Следующим
шагом, требующим адаптации, является удаление из документов стоп-слов. Для
этой цели был использован стоп-словарь, составленный на основе данных из
Национального корпуса русского языка (НКРЯ) [13], который включает
наиболее частотные предлоги, частицы, местоимения, междометия, некоторые
вводные слова и конструкции, а также количественные и порядковые
числительные, цифры и символы латиницы [14]. На основе уже имеющихся в
пакете методов, предполагающих удаление стоп-слов из исходных текстов для
других языков, был разработан отдельный метод для русскоязычного стопсловаря.
3.2. Ход эксперимента, полученные данные
Эксперименты проводились на материале четырех корпусов русскоязычных
текстов
по
ракетостроению
и
аэрокосмическим
исследованиям,
представляющих научный, публицистический, официально-деловой и
художественный функциональные стили [15]. Объем каждого из корпусов
составляет примерно 500 тыс. с/у, суммарный объем обработанных текстов, тем
самым, оценивается в 2 млн с/у.
Для корпусов с текстами научного и художественного стилей автоматически
были получены списки ключевых выражений (по 40 наиболее частотных
биграмм, ранжированных по значению MI, общее число — 80 биграмм). Корпус
был поделен на обучающую и тестовую выборки. Для обучения использовались
корпусы с текстами научного и художественного стилей, а для тестирования —
публицистического и официально-делового. В обучающей выборке были
размечены употребления выражений, совпадающих с биграммами из списка. В
результате эксперимента для каждого документа из тестовой выборки было
выделено 20 ключевых слов и словосочетаний.
Примеры ключевых слов и словосочетаний, выделенных KEA: система
координат, космический аппарат, система управления, источник энергии,
анализировать причину, принимать решение, солнечная система, планета,
химический состав, удельный импульс, сила тяги, решать проблему, процесс
горения, стартовая масса, компонент топлива, слой атмосферы, космический
корабль, космический аппарат, и т.д.
3.3. Оценка качества автоматического выделения ключевых слов и
словосочетаний
Существует два основных подхода к оценке качества ключевых слов и
словосочетаний, выделенных автоматически, и они оба, так или иначе,
предполагают участие авторов или информантов.
Первый подход основан на вычислении стандартных метрик из области
информационного поиска — точности и полноты. Автоматически выделенная

ключевая информация сравнивается с так называемым «золотым стандартом»,
представленным ключевыми словами и словосочетаниями, назначенными
автором. Разумеется, у этого подхода есть свои недостатки. Во-первых,
выделенные автором выражения не всегда наблюдаются в тексте. Во-вторых, их
выбор порой преследует также несколько иные цели помимо краткого описания
документа. В-третьих, далеко не каждый документ содержит ключевые слова,
выделенные вручную. В-четвёртых, зачастую авторы выбирают лишь
небольшое количество слов и словосочетаний.
Второй подход — оценивание автоматически выделенных ключевых
выражений экспертами. Каждому информанту представляют документ и список
ключевых слов и словосочетаний, выделенных для него автоматически, и
предлагают тем или иным образом оценить релевантность каждого выражения
по отношению к данному документу. Как и у предыдущего, у этого подхода
тоже есть свои недостатки. Главным из них является, разумеется,
субъективность оценки и её последующая объективизация. Второй
существенный недостаток — очевидная трудность проведения эксперимента
для объёмных документов.
Нами был выбран полностью автоматизированный способ оценки работы
данного алгоритма. Во-первых, как отмечалось выше, 40 наиболее частотных
биграмм для каждого документа из обучающей выборки были получены
автоматически. Во-вторых, непосредственная оценка результатов применения
алгоритма KEA при работе с экспериментальными корпусами осуществлялась с
помощью построения тематических моделей для каждого из корпусов. Этот
выбор обусловлен тем, что ключевые слова и словосочетания, составляющие
семантическое ядро корпуса, находят соответствие в тематической модели
корпуса (т.е. компоненты n-грамм должны быть представлены в составе
кластеров, отражающих распределение слов по темам и тем по документам
корпуса). При построении тематических моделей корпусов использовался
алгоритм LDA (Latent Dirichlet Allocation) в пакете GenSim для Python [12]. В
каждой тематической модели отбирались 200 статистически значимых лемм (по
10 из 20 тем), далее фиксировалось их наличие/отсутствие в списках ключевых
выражений. За единичными исключениями все леммы в составе тем
обнаружены в верхней трети списка ключевых выражений, которая оценивается
как наиболее информативная.

4. Заключение
Таким образом, в ходе данного эксперимента была осуществлена адаптация
KEA посредством дополнения алгоритма инструментами для работы с русским
языком. Также, мы осуществили оценку эффективности работы KEA на
материале русского языка с помощью построения тематических моделей и
выяснили, что полученные данные дают основания считать результаты работы
алгоритма KEA приемлемыми, а сам алгоритм в русскоязычной модификации
пригодным для использования в лингвистических исследованиях.
Сравнив результаты работы KEA на материале русского языка с выдачей
неоднократно протестированных и доказавших свою эффективность
тематических моделей, построенных с помощью алгоритма LDA, мы можем

прийти к выводу, что адаптированный нами алгоритм KEA является весьма
полезным инструментом в автоматическом определении тематики текста. Он
показывает удовлетворительные результаты, что подтверждается нахождением
выделенных им ключевых слов и выражений в наиболее информативной части
построенных тематических моделей, которые, как указывалось выше, призваны
отражать статистическое распределение слов по темам и тем по документам
корпуса.
Что касается перспектив дальнейшего исследования, то планируется
сравнение KEA с другими алгоритмами и экспертиза результатов с участием
информантов.
Исследование поддержано грантом РФФИ № 16-06-00529 «Разработка
лингвистического комплекса для автоматического семантического анализа
русскоязычных корпусов текстов с применением статистических методов»
(2015–2018 гг.).