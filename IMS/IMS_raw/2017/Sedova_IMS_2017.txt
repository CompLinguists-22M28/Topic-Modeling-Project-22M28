Тематическое моделирование русскоязычных текстов
с опорой на леммы и лексические конструкции
А.Г. Седова, О.А. Митрофанова
Санкт-Петербургский государственный университет,


1. Введение
В настоящее время в корпусной лингвистике активно развивается направление
вероятностного тематического моделирования, опирающегося на статистическую
обработку текстов. Тема (topic, latent topic), или скрытый паттерн (hidden pattern) —
это дискретное вероятностное распределение в пространстве слов заданного словаря
[1]. Формально говоря, темы являются результатом би-кластеризации, то есть
одновременной кластеризации и слов, и документов по их семантической близости.
Тематическим моделированием при этом называется восстановление вероятностных
распределений всех тем в тексте, рассматриваемом как случайная независимая
выборка слов («мешок слов»), порожденная некоторыми темами. При этом
достаточно небольшое число тем может породить документ, состоящий из большого
количества слов. Порядок тем при различных запусках алгоритма может
варьироваться,
что
обусловлено
свойством
неупорядоченности,
или
перестановочности (exchangeability) тем.
Тематическая модель описывает связи между словами и темами, документами и
темами с помощью смеси дискретных распределений. Таким образом, тематическая
модель выступает как средство обобщения и систематизации информации из
больших текстовых коллекций и позволяет выявить скрытые структуры и неявные
зависимости в данных. Тематическое моделирование широко применяется для

решения задач информационного поиска и машинного перевода, для кластеризации
изображений, распознавания объектов и рукописного текста, определения
эмоциональной окраски текстов, выявления и анализа различных временных трендов,
осуществления автоматического аннотирования и индексирования документов [2].
Одним из наиболее распространенных методов построения тематических моделей
является метод латентного размещения Дирихле (Latent Dirichlet Allocation, LDA) [3].
Данный алгоритм опирается на априорное распределение Дирихле и использует в
своей работе модель «мешка слов» (bag-of-words) — модель для анализа текстов,
которая учитывает только частоту слов, но не их порядок; данная модель хорошо
подходит для многих методов тематического моделирования, поскольку она
позволяет обнаруживать неявные взаимосвязи между словами с учетом полисемии.
Метод LDA осуществляет мягкую кластеризацию и предполагает, что каждое слово в
документе порождено некоторой латентной темой, определяющейся вероятностным
распределением на множестве всех слов текста. В нашем исследовании мы
пользовались именно этим алгоритмом.
На вход практически любой тематической модели поступает корпус текстов,
каждый из которых является отдельным документом. Результатом работы модели
является список тем, выявленных в корпусе и представленных списком первых,
наиболее характерных n-слов для каждой рассматриваемой темы. В базовых
алгоритмах тематического моделирования темы представлены исключительно
униграммами (например, «система частота время измерение объект скорость
дальность метод параметр обработка», «свойство расстояние отраженный излучать
отражение радиолокационный земля объект мощность отражать»). В первую очередь
это происходит благодаря использованию модели «мешка слов», не учитывающий
линейной зависимости между словами внутри предложения. Зачастую это влечет за
собой ухудшение точности и повышает сложность содержательной интерпретации
выделяемых тем, особенно в случае некомпозиционных словосочетаний, значение
которых не сводится к сумме значений входящих в них слов: например, «железная
дорога» не сводится к значению слов «железная» и «дорога» соответственно [4].
Таким образом, добавление в темы расширение тем за счет n-грамм представляет
собой актуальную исследовательскую задачу.
В последнее время было предложено несколько подходов к решению данной
проблемы [5, 6]. Однако многие из них снижают качество модели или же излишне
усложняют её [4]. В данной работе была предпринята попытка предложить новый
метод, который бы действительно упрощал интерпретацию тем и повышал их
точность, оставаясь при этом понятным и простым для реализации.

2. Алгоритм добавления биграмм в тематические модели
2.1. Возможные решения задачи
В зависимости от используемого алгоритма предлагаемые для решения данной
задачи алгоритмы делятся на две группы:
Ниже приведен пример оформления списка:
− алгоритмы, представляющие собой унифицированную тематическую модель,
в рамках которой словосочетания выделяются в тексте одновременно с
темами;

алгоритмы, выделяющие многословные выражения на этапе предобработки
текста.
Большинство существующих алгоритмов относятся к первой группе. Одним из
них является, например, биграммная тематическая модель (bigram topic model).
Данная модель является иерархической порождающей, и при её работе в качестве
−

основополагающего используется предположение о зависимости появления слова
зависит исключительно от
интересующим нас словом:

слова

стоящего

,
где {

} – гиперпараметры модели,

непосредственно

перед

,

— частотность слова

,

—

частотность словосочетания
[5]. Однако недостатком данного алгоритма
можно считать то, что он позволяет выделять в документах только темы, состоящие
исключительно из биграмм, что может навредить точности и полноте тем.
Другим примером тематической модели, включающей в себя выделение
словосочетаний в тексте одновременно с темами, является скрытая тематическая
марковская модель с применением латентного размещения Дирихле (Hidden Markov
Model with Latent Dirichlet Allocation, HMM-LDA). В данной модели строится
совместное описание синтаксиса и семантики текста с помощью разбиения каждого
предложение на функциональные слова, которые порождаются с помощью скрытой
марковской модели, (таким образом, описываются локальные закономерности), и на
термины, генерируемые тематической моделью LDA (так дается глобальное
тематическое описание документа) [7]. Модель состоит из последовательности
переменных-слов
тематических переменных
бинарных классификаций

указывающих, образуют ли данное слово и

предыдущее словосочетание. Значение
слове

выбирается, основываясь на предыдущем

, исходя из распределения

образуют словосочетание и слово

и последовательности

. Если

, то слова

и

анализируется в семантическом аспекте, т. е. на

основании тематического распределения

:
.

Если же

, то слово порождается из распределения

:

.
Несомненным достоинством унифицированных тематических моделей является
их логическое теоретическое обоснование. Однако к их минусам можно отнести
большое количество параметров, нуждающихся в настройке. Например, число
параметров у Биграммной Тематической Модели равно W2T, в то время как у

базовой модели LDA — WT, где W — размер словаря (т.е. число уникальных слов и
словосочетаний корпуса), T — число выделенных тем.
Одним из наиболее распространенных алгоритмов, относящихся ко второму типу,
то есть позволяющих выделить биграммы на этапе предобработки текста, является
алгоритм, предложенный в работе [8]. В данном алгоритме коллокации выявляются
на этапе предварительной обработки текста, упорядочиваются в соответствии с
ассоциативной мерой T-Score:

где TF(xy) — частотность словосочетания xy, TF(x) и TF(y) — частотность слов x
и y соответственно, |W| - число различных слов в коллекции.
Далее наиболее удачные полученные словосочетания объединяются в один токен
и добавляются в корпус, заменяя униграммы. Таким образом, в процессе собственно
построения тематической модели (в данном случае, LDA) и построения модели
«мешка слов», они рассматриваются наряду с другими униграммами как токены.
Другим алгоритмом такого типа являются алгоритмы PLSA-SIM и PLSA-ITER,
предложенные в [9]. Оба данных алгоритма являются усовершенствованными
версиями одной из базовых моделей PLSA — вероятностного латентного
семантического анализа, основанной на введении слоя скрытых переменных для
описания тематик документов из корпуса текстов [10]. – и также используют в своей
работе модель мешка слов. Идея, которая легла в основу алгоритма PLSA-SIM,
следующая: в любых текстах существует большое количество слов и коллокаций,
семантически и лексически близких: например, бюджетный, бюджетные расходы,
бюджетные средства, бюджетные доходы [4]. В рамках данного алгоритма при
выделении тем подобные словосочетания относятся к одной теме. Если же подобные
слова и словосочетания никогда не встречаются в рамках одного документа, то для
них выполняется стандартный алгоритм PLSA.
Дальнейшим усовершенствованием алгоритма PLSA-SIM является итеративный
алгоритм PLSA-ITER, в рамках которого рассматриваются самые частотные
униграммы, представляющие темы, и из них составляются биграммы. В качестве
примера авторами приводится биграмма ценный бумага, которая может быть
составлена, если в некой теме среди первых n слов окажутся униграммы ценный и
бумага. В [4] рассматриваются первые 10 униграмм, из которых далее образуются
биграммы и добавляются в тематические модели. Помимо этого, в данной модели,
как и в предыдущей, учитывалась частеречная принадлежность слов: в выделении
тем участвовали только существительные, прилагательные, глаголы и наречия, а в
формировании
биграмм
для
русского
языка
следующие
коллокации:
существительное + существительное в родительном падеже, прилагательное +
существительное.
Подход, предполагающий предварительное выделение биграмм в текстовых
коллекциях, возможно, не имеет такого изящного теоретического обоснования,
однако позволяет строить алгоритмы, являющиеся гораздо более простыми в
применении. В первую очередь это достигается за счет того, что количество
настраиваемых параметров в данных моделях равен их количеству в исходных

моделях (как правило, LDA или PLSA). Недостатком данного подхода можно назвать
повышение перплексии, что ведет к ухудшению обобщающей способности
выявленной модели.
Предлагаемый нами метод можно отнести ко второму типу, поскольку он
предполагает автоматическое выделение биграмм на этапе предварительной
обработки текста, а затем уже их последующее добавление в тематические модели.
2.2. Используемые корпусные данные
Для проведения эксперимента были выбраны два корпуса русскоязычных текстов:
− корпус специальных текстов по радиоэлектронике, ракетостроению и
технике [11]; общий объем корпуса: 526 648 словоформ;
− корпус русскоязычных специальных текстов на лингвистическую тематику
из лингвистического энциклопедического словаря (ЛЭС) под редакцией
В.Н.Ярцевой и энциклопедии «Кругосвет» [12]; общий объем корпуса:
1 333 546 словоформ.
Причин выбора текстов именно научного стиля несколько. Во-первых,
специфическими чертами подобных текстов являются а) значительная лаконичность,
однозначность, б) малая экспрессивность, метафоричность и синонимия, что
несомненно позволяет упростить построение и интерпретацию тематических
моделей, повысить их точность и снизить долю шума. Во-вторых, их характерной
чертой является повышенное содержание терминов. Термины (в рамках данного
исследования: отдельные слова и двухсловные сочетания) всегда существуют в
рамках терминосистемы, обладают фиксированным терминологическим значением и,
как правило, обозначают какой-то предмет, явление или, в крайнем случае, процесс.
С одной стороны, это облегчает процесс тематического моделирования, поскольку
тематика документов из специального корпуса текстов предсказуема. С другой
стороны, регулярность и воспроизводимость терминов в рамках одного текста
упрощает выделение биграмм. Также не стоит упускать из виду то, что, как правило,
термины — это номинативная лексика, терминосочетания формируются из
существительных, глаголов и прилагательных, и коллокаты этих частей речи
составляют наиболее частотные биграммы.
На этапе предварительной обработки текстов из них удаляются нетекстовые
символы и сокращения (в рамках данного эксперимента было принято решение об
удалении всех слов, длина которых составляет менее 3 символов). Помимо этого, из
текстов исключаются слова, входящие в список стоп-слов на основе словарей
служебных слов и оборотов НКРЯ, а также 98 наиболее частотных глаголов и
отвлеченных существительных (например, использовать, позволять, наличие,
отсутствие и так далее).
Далее была произведена лемматизация текстов и автоматическое разрешение
морфологической неоднозначности с помощью морфологического анализатора
русского языка MyStem 3.0 [13].
После прохождения этапа предварительной обработки текстов объемы корпусов
оказались следующими:
− корпус текстов по радиоэлектронике, ракетостроению и технике: 216 613
леммы;
− корпус текстов на лингвистическую тематику: 1 246 590 лемм.

2.3. Описание алгоритма
На первом этапе работы алгоритма в исследуемом корпусе были выявлены
биграммы с использованием модуля Phrases, входящего в состав библиотеки Gensim1.
Данный модуль, используя модель «мешка слов», автоматически определяет
наиболее часто встречающиеся в документах многословные словосочетания.
Параметр, отвечающий за принятие решения о формировании биграммы (threshold)
основан на совместной встречаемости слов. Слова а и b считаются биграммой, если:

где N – общий размер словаря.
В наших экспериментах в качестве документов рассматривались предложения из
корпуса, поскольку наибольший интерес вызывает совместная встречаемость слов
именно в пределах синтагм. Обучение модуля проводилось непосредственно на
корпусе текстов, с которым велась далее работа. Технически алгоритм был
реализован на языке программирования Python.
В процессе работы алгоритма в корпусе текстов было образовано 14 542 биграмм
для корпуса текстов по радиоэлектронике, ракетостроению и технике и
187 008 биграмм для корпуса текстов на лингвистическую тематику. Соотношение
же с общим объемом корпусов составляет: 13,4% для корпуса текстов по
радиоэлектронике, ракетостроению и технике и 30.0% для корпуса текстов на
лингвистическую тематику. Возможных причин бόльшему количеству выделенных
биграмм во втором корпусе несколько: во-первых, роль играет объем корпуса,
поскольку от него напрямую зависит параметр threshold: при увеличении объема
число биграмм также увеличивается. Во-вторых, стоит отметить разницу в
лексической специфике собранных корпусов. Корпус текстов на лингвистическую
тематику более однородный; лексическое наполнение входящих в него текстов более
однообразно, наблюдается бόльшее количество высокочастотных терминов.
Напротив, корпус по радиоэлектронике, ракетостроению и технике представляет
собой тематически более разнородную выборку текстов и изобилует
низкочастотными терминами. Общая частота употребления терминов в корпусе
снижается за счет лексического разнообразия, и поэтому меньшее количество
униграмм проходит порог формирования биграммы. Помимо этого, в соответствии с
установленными нами параметрами униграммы, встречающиеся в текстах менее двух
раз, вовсе не рассматриваются при выделении биграмм; поэтому, вероятно,
некоторое количество потенциальных биграмм, которые были образованы из
низкочастотных терминов, не попали в конечный список.
Пример составов корпусов до предварительной обработки, после первично
обработки, а также после работы алгоритма по выделению биграмм приведены в
табл.1 и табл. 2.


…При qys>\ ошибка Х — Хо линейно связана с величиной
Z'(Хо), которая может считаться гауссовской в силу
нормальности Z(X) в рассматриваемых условиях. Как
выясняется, разработанная в 4.7 методика расчета
потенциальной точности, т. е. дисперсий ОМП, оказывается
удовлетворительной только при условии, что превышение
сигнала над шумом настолько велико, что наблюдатель
вправе полагать разброс X относительно Хо полностью
укладывающимся в пределы линейного участка производной
ФН, смещенной в точку Х = Х0. Для этого прежде всего
необходимо, чтобы побочные (шумовые) выбросы на 4.7, в не
превосходили основного пика, обусловленного ФН сигнала…
считаться линейно условие гауссовский величина связанный
сила выясняться расчет точность потенциальный методика
удовлетворительный точка полностью предел вправе шум
производная дисперсия условие превышение укладываться
смещать разброс наблюдатель настолько сигнал линейный
участок омп большой побочный шумовой выброс необходимо
обусловливать пик сигнал основный превосходить
линейно условие гауссовский_величина связанный сила
выясняться расчет_точность потенциальный методика
удовлетворительный точка полностью предел вправе шум
производная_дисперсия условие_превышение укладываться
смещать разброс наблюдатель настолько_сигнал линейный
участок
омп
большой
побочный_выброс
шумовой
необходимо
обусловливать
пик
сигнал
основный
превосходить

Анализируя примеры из корпуса, можно заметить, что в результате работы
алгоритма было образованно несколько несомненно правильных биграмм, хоть и не
стоящих в данном отрывке в непосредственной близости: например,
гауссовский_величина,
побочный_выброс,
расчет_точность
и
норма_произносительный,
норма_орфоэпический,
национальный_язык,
норма_складываться. Остальные же обобщенные униграммы в данном отрывке как
таковыми биграммами не являются.


…Кодифицированная норма часто отстаёт от реально
сложившейся. Орфоэпия складывается одновременно с
формированием национального языка, когда расширяется
сфера действия устной речи, развиваются новые формы
публичной речи. В разных национальных языках процесс
становления орфоэпических норм проходит по-разному.
Орфоэпические нормы могут пройти несколько этапов,
прежде чем стать нормами национальными языка…

Отрывок корпуса до
предварительной
обработки, перед запуском
алгоритма по выделению
биграмм

Отрывок корпуса после
работы алгоритма по
выделению биграмм

кодифицированный
часто
отставать
реально
норма
складываться речь язык новый устный формирование
национальный
публичный
одновременно
действие
развиваться форма складываться сфера орфоэпия расширяться
язык
разный
национальный
становление
норма
орфоэпический
проходить
разному
процесс
язык
национальный норма орфоэпический проходить этап русский
особенность
Кодифицированный
часто_отставать
реально
норма_складываться
речь
язык
новый
устный_формирование
национальный_публичный
одновременно действие развиваться форма складываться
сфера_орфоэпия расширяться язык разный национальный
становление норма_орфоэпический проходить разному
процесс
язык
национальный
норма_орфоэпический
проходить этап русский особенность

На втором этапе работы алгоритма была построена тематическая модель
экспериментального корпуса. Как уже было упомянуто ранее, нами было принято
решение использовать для этого вероятностную тематическую модель латентного
размещения Дирихле [3], включенную в пакет для анализа данных Scikit-Learn2.
Эмпирическим путем были установлены параметры, позволяющие выделить темы
наиболее точно: количество итераций алгоритма — 200, количество тем — 20,
количество слов, представляющих каждую тему — 10 первых слов. Последний
параметр неслучайно был выбран именно таким: исследования показали, что именно
10 первых слов содержат в себе 30% информации о теме, распределенной в других
словах, что является достаточным для достаточно полного представления темы [14].
Также стоит отметить, что при построении тематической модели не учитывались
слова, встретившиеся менее, чем в двух документах, а также высокочастотные слова
— в данном случае, содержащиеся более чем в 80% документов.
На третьем этапе полученные темы были заново обработаны с помощью модуля
Phrases, что позволило выделить еще некоторое количество биграмм. Полученные
конечные результаты представлены в табл. 3 и табл. 4.



Проанализировав полученные результаты, можно заметить, что большинство
выделенных биграмм действительно образуют логичные словосочетания, такие как:
линейное пространство, ширина спектра, случайная величина, преобразование
Фурье, суммарный канал, радиолокационная цель, кодовое слово, проверка гипотезы.
Остальные же выделенные биграммы вполне объяснимы: например, биграмма

потребитель_исз был выделен, вероятно, вследствие того, что слова исз
(искусственный спутник Земли) и потребитель (в значении исследователь,
наблюдатель) часто встречаются в таких схожих контекстах, как расстояние между
потребителем и ИСЗ, скорость ИСЗ относительно потребителя и так далее. Также
слова обзор и рлс (радиолокационные станции), хоть и не встречаются в тексте
стоящими рядом, во многих контекстах встречаются в непосредственной близости:
например, РЛС дальнего обзора, РЛС ближнего обзора, обзорные РЛС и т.п.
Результаты, полученные на втором корпусе текстов, отличаются от полученных
на первом в основном тем, что в темах выделилось значительно меньше биграмм:
влияние_оказывать и словарный_статья, и обе данные биграммы являются
верными. Общее количество выделенных в темах биграмм также меньше, чем в
предыдущем случае, однако значительная их часть являются достаточно точными: из
них можно образовать логичные словосочетания латинская письменность,
алфавитная письменность, падежная форма, словарное толкование. Биграмм
ударение_позиция также нельзя назвать случайным: составляющие его униграммы
часто встречаются в одном предложении, например, «… фиксированное ударение
ориентируется на крайние позиции в слове — либо на его начало, либо на конец…»
или даже в непосредственной близости: «…Особенность фонетики собственно
алюторского диалекта — противопоставление по долготе в системе гласных, …,
динамическое позиционное ударение…». Униграммы число и местоимение, по всей
видимости, были объединены также на основании частой совместной встречаемости
в одном предложении (несложно представить такие контексты, описывающие формы
местоимений); однако нельзя утверждать, что они образуют верную биграмму.

3. Заключение
В настоящей статье был предложен алгоритм для автоматического выделения
биграмм на этапе предварительной обработки текста и их последующего добавления
в тематические модели. За основу была взята вероятностная модель латентного
размещения Дирихле. Разработанный алгоритм был проверен на двух корпусах
специальных текстов.
Достоинство предложенного алгоритма заключается в том, что он позволяет
выявлять биграммы в корпусах текстов, существенно не усложняя модель и не
требуя внедрения дополнительных параметров. Стоит также отметить, что, благодаря
очевидному удобству применения данного метода, он может считаться
универсальным и применим для выделения словосочетаний в текстах разных стилей
и типов (как научных, так и художественных, официально-деловых и
публицистических).
В перспективе планируется усовершенствовать выделение биграмм, используя
частеречную разметку корпуса. В большинстве своем правильно выделенные темы
формируются именно из существительных и именных групп [15], поэтому в
дальнейшем
планируется
формировать
биграммы
в
корпусе
текстов
преимущественно в соответствии с моделями существительное + существительное
в родительном падеже, существительное + прилагательное; также не исключено
добавление коллокаций существительное + глагол, поскольку зачастую такие
словосочетания также являются характерными для специальных текстов. Наряду с
этим, ставится задача полного исключения формирования таких ошибочных

сочетаний, как, например, существительное + наречие или прилагательное + глагол.
Также планируется приведение биграмм из лемматизированной формы к
согласованным словосочетаниям путем их повторного поиска в корпусе текстов и
замены на исходные формы.
Исследование поддержано грантом РФФИ № 16-06-00529 «Разработка
лингвистического комплекса для автоматического семантического анализа
русскоязычных корпусов текстов с применением статистических методов» (2015–
2018 гг.).