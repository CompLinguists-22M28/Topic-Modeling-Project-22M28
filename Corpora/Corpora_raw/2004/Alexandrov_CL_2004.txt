П. Макагонов, М. Александров, А. Гельбух
ФОРМУЛЫ  ПРОВЕРКИ  ПОДОБИЯ  СЛОВ   
С  ОБУЧЕНИЕМ  НА  ПРИМЕРАХ:  
ПОСТРОЕНИЕ  И  ПРИМЕНЕНИЕ
     [Работа выполнена при поддержке грантa 39011a (CONACyT) 
Национального Совета по Науке и Технологиям Мексики и гранта 20020237 
(CGPI-IPN) Национального Политехнического Института Мексики. Данная 
статья является переработанным и расширенным вариантом статьи 
Alexandrov M., Blanco X., Makagonov P. Testing Word Similarity: Language 
Independent Approach with Examples from Romance // Farid Meziane et al. (eds.) 
Natural Language Processing and Information Systems. Lecture Notes in Computer 
Science, № 3136. Springer, 2004. P. 223-234.]
    1. Введение
    Для заданного текста (или корпуса текстов) мы рассматри-
ваем задачу группировки слов, имеющих одно и тоже базовое 
значение, как, например, sad, sadly, sadness, sadden, saddened и т.д. 
Алгоритм, решающий подобную задачу, может делать ошибки 
двух типов: объединять слова, которые не являются подобными, 
и не объединять слова, которые в действительности подобны. Для 
удобства будем называть их "ложная тревога" и "пропуск" соот-
ветственно, как это принято в задачах обнаружения сигналов. В 
рамках подхода, развиваемого в данной статье, мы можем допус-
тить определенный уровень таких ошибок, так как нашей целью 
является улучшение процедур поиска информации, а не точный 
лингвистический анализ.
    Существует два подхода к выделению слов, имеющих одно 
базовое значение. При лемматизации все слова приводятся к 
стандартной словарной форме (единственное число и мужской 
род для существительных, инфинитив для глаголов и т.п.) с при-
менением правил морфологии и больших морфологических сло-
варей, после чего подобные слова объединяются в группы. Этот 
подход обеспечивает практически стопроцентную точность на из-
вестных словах, и несколько процентов ошибок на словах, отсут-
ствующих в словаре [Gelbukh A. Exact and Approximate Prefiх Search under 
Access Locality Requirements for Morphological Analysis and Spelling Correction // 
Computacion y Sistemas. 2003. Vol. 6. № 3. P. 167-182; Gelbukh A., Sidorov G. 
Approach to Construction of Automatic Morphological Analysis Systems for Inflective 
Languages with Little Effort // Computational Linguistics and Intelligent Text 
Processing (CICLing-2003). Lecture Notes in Computer Science, № 2588. Springer, 
2003. P. 215-220].
    В процедуре стемминга [Porter M. An Algorithm for Suffix Stripping // 
Program. 1980. № 14. P. 130-137] все слова усекаются до их корневой 
части, которая отражает их инвариантное значение, т.е. sad, sadly, 
sadness, sadden, saddened заменяются на sad-. Затем проводится 
объединение одинаковых слов. Хотя этот метод много проще (так 
как он использует только списки суффиксов и правила удаления 
суффиксов), он обеспечивает относительно высокую точность до 
95%. Но даже такой экономичный метод требует формализации и 
программирования больших наборов правил, связанных с выде-
лением и удалением суффиксов. Это становится непростой зада-
чей при обработке больших массивов многоязыковых и много-
тематических коллекций документов.
    В этой статье описывается другой подход, который сводится 
к построению эмпирических формул, учитывающих только сов-
падение начальных частей сравниваемых слов. Для построения 
формул используются примеры слов, которые даются экспертом. 
Ранее мы рассмотрели этот подход в ряде работ [Alexandrov M., 
Blanco X., Makagonov P. Op. cit.; Makagonov P., Alexandrov M. Constructing 
Empirical Formulas for Testing Word Similarity by the Inductive Method of Model 
Self-Organization // E. Ranchhold and N. Mamede (eds.) Advances in Natural 
Language Processing. Lecture Notes in Artificial Intelligence, № 2379. Springer, 
2002. P. 239-247]. В данной статье мы более детально описываем 
шаги метода и предлагаем различные алгоритмы построения 
частотных списков слов, где применяются формулы подобия. 
Главные преимущества предложенного метода - простота и 
гибкость, а главный недостаток - более низкая точность, чем у 
традиционных методов.
    2. Постановка задачи
    2.1. Формулы для проверки подобия слов
    Основное назначение формул, которые должны быть постро-
ены - это проверка гипотезы о подобии слов. Мы рассматриваем 
только те языки, где основа слова располагается в его начале (а не 
в конце), что справедливо для большинства европейских языков.
    В формуле будут использованы следующие характеристики 
пары слов: 
n: суммарная длина финальных частей двух слов,
y: длина общей начальной части,
s: общее число букв в сравниваемой паре слов,
очевидно, что <object> 2y + n = s. </object> Например, для слов 
sadly и sadness <object>n = 6, y = 3, </object> и <object>s = 
12</object>.
    Мы рассматриваем каждую формулу как модель из некото-
рого класса. Модели одного класса отличаются числом парамет-
ров, которое определяет сложность модели. Нашей задачей явля-
ется выбор модели оптимальной сложности.
    В статье рассматривается следующий класс моделей для 
принятия решения о подобии двух слов: два слова считаются по-
добными, если относительное число букв в их различающихся 
финальных частях меньше некоторого порога, зависящего от чис-
ла совпавших букв в начальной части этих слов:
    <object>
      ? F (y),       F(y) = a + b1y + b2y2 + ... + bkyk,                   (1)
  </object>
где n, s, и y были определены выше, F(y) - модельная функция. 
Такое представление модельной функции является достаточно 
общим, так как любая гладкая функция может быть представлена 
сходящимся полиномиальным рядом.
    Очевидно, такие модели имеют две степени свободы, n и y, 
по отношению к характеристикам пары слов. Это позволяет более 
точно учесть статистические свойства языка чем модели с одной 
степенью свободы, такие как <object>n/s ? C, </object> где С - 
некоторая константа или <object> n/s ? F(y/s). </object>
    Конечно, могут быть рассмотрены и другие классы моделей, 
например, такие, где задается какая-либо функция веса для пози-
ций букв в слове. В настоящей статье мы будем рассматривать 
только модели вида (1), и нашей первой задачей будет выбор наи-
лучшей формы модельной функции, т.е. степени полинома. За-
тем, когда модель будет выбрана, будет легко найти ее оптималь-
ные параметры - коэффициенты полинома.
    2.2. Ограничения на использование метода
    Очевидно, что наш подход для проверки подобия слов не 
применим к словам с нестандартным словоизменением. Речь 
идет, прежде всего, о глаголах. Действительно, невозможно по-
строить какую-нибудь простую формулу, которая бы обнару-
живала подобие между такими английскими глаголами как go и 
went. Такая же ситуация имеет место и для других языков, в 
частности, для испанского, который мы используем в примерах 
этой статьи.
    Так как эмпирическая формула строится на основе статис-
тических закономерностей языка, это ведет к упомянутым выше 
ошибкам 1-го и 2-го типа. Подстраивая параметры модели, мы 
можем управлять балансом между этими двумя типами ошибок. 
В частности, положив, 
  <object>
  ? -1    или       ? 1,
</object>
мы будем отвергать подобие любых слов или рассматривать все 
слова как подобные. В первом случае получим 0% ошибок пер-
вого 1-го типа и 100% ошибок 2-го типа, а во втором 0% ошибок 
2-го типа и 100% ошибок 1-го типа.
    Некоторые специфические ошибки могут быть вызваны из-
лишне широким толкованием базового смысла слов. Будем назы-
вать их ошибками 3-го типа. Как пример рассмотрим слова move, 
moving, moveable и moveability. Последние два слова могут рас-
сматриваться как имеющие тот же базовый смысл, что и первые 
два, или же как отличающиеся от них дополнительным значе-
нием способности к движению. Любая расширенная интерпре-
тация базового смысла экспертом, который готовит примеры, бу-
дет приводить к формулам с высоким уровнем ошибок 1-го типа. 
Очевидно, что ошибки 3-го типа более специфичны для доста-
точно узких предметных областей. Например, слова relate и 
relativity могут рассматриваться как подобные в обычных тек-
стах, но не в текстах по физике.
    3. Построение эмпирической формулы
    3.1. Индуктивный метод самоорганизации моделей
    Индуктивный метод самоорганизации моделей (ИМСОМ) [ 
Ивахненко A. Руководство по типовым программам моделирования. Киев, 1980] 
позволяет определять модель оптимальной сложности для описа-
ния различных процессов или явлений. Метод выбирает квазиоп-
тимальную модель в выбранном классе моделей, используя экс-
периментальные данные. Метод не предназначен для нахождения 
наиболее оптимальной модели вообще, но использует сравнение 
моделей, которые усложняются от шага к шагу. По этой причине 
он называется индуктивным.
    Метод предполагает выполнение следующих шагов.
    1.	Эксперт определяет последовательность моделей от са-
мых простых к более сложным.
    2.	Экспериментальные данные делятся на два набора: обу-
чающую и контрольную выборку. Это деление проводится вруч-
ную или автоматически.
    Для каждого вида модели определяются ее наилучшие 
параметры отдельно для обучающей и контрольной выборок. 
Здесь могут использоваться любые внутренние критерии 
[Внутренние критерии используют одни и те же данные как для оценки 
качества модели, так и для определения значений ее параметров. Внешние 
критерии для этих целей используют различные данные. Обычно внешние 
критерии строятся таким образом, чтобы их наилучшее значение было 
нулевым, а сами они были неотрицательными функциями] согласованности 
между моделью и данными, например, критерий наименьших 
квадратов.
    3.	Обе модели сравниваются на основе внешних критериев 
(см. ниже), таких как критерии регулярности и несмещенности.
    4.	Проверяется, достигли ли внешние критерии (или наибо-
лее важный из них) устойчивого оптимума. В этом случае поиск 
заканчивается. Иначе рассматривается более сложная модель и 
процесс повторяется с шага 3.
    Поясним, почему внешние критерии достигают некоторого 
оптимума (минимума). Предполагается, что экспериментальные 
данные содержат: 
а) регулярную компоненту, определяемую структурой модели и 
б) случайную компоненту - шум.
    Очевидно, что модель должна быть способна отражать изме-
нения в регулярной компоненте. Когда модель слишком проста 
(содержит мало параметров), она слабо чувствительна к этой 
регулярной компоненте и нечувствительна к шуму. Когда модель 
слишком сложна (содержит много параметров), она хорошо отра-
жает как регулярную компоненту, так и случайную. В обоих слу-
чаях значения критерия оказываются большими. Таким образом, 
мы ожидаем найти точку, где критерий достигает своего миниму-
ма. Принцип самоорганизации состоит в том, что внешний крите-
рий проходит свой минимум, когда сложность модели постепен-
но увеличивается.
    Для того чтобы применить ИМСОМ к задаче построения 
формул, мы рассматриваем экстремальные случаи уравнения (1), 
т.е. вместо неравенств рассматриваем равенства. Это дает сис-
тему линейных уравнений относительно неизвестных параметров 
<object> a, b1, b2, ..., bk:
    ni/si  = a + b1yi + b2yi2 + ... + bkyik,     i = 1, ..., m                  (2)
    </object>
Здесь n, s и y определены в разделе 2.1., а i - число примеров, 
подготовленных экспертом по данному языку. Конечно, число 
уравнений m должно быть больше, чем число переменных (k+1), 
чтобы отфильтровать шум. Известно, что для хорошей фильтра-
ции следует обеспечить по крайней мере трехкратное превыше-
ние числа уравнений над числом переменных, при этом уровень 
шума уменьшается приблизительно на порядок.
    Примеры, подготовленные экспертом (носителем данного 
языка), могут рассматриваться как экспериментальные данные. 
Возьмем, например, слова hoping и hopefully. Эти два слова име-
ют очень короткую общую часть hop- и длинные отличающиеся 
части -ing и -efully. Для этой пары имеем уравнения: 
    <object>9/15 = a + 3b1 </object>              для линейной модели
    <object>9/15 = a + 3b1 + 9b2 </object> для квадратичной 
модели и т.д.
    Далее, согласно методологии ИМСОМ, для различных k, на-
чиная с <object>k = 1</object>, находим наилучшее решение (2) 
для заданного внутреннего критерия, и затем внешний 
интегральный критерий (критерии) проверяется на достижение 
минимума.
    3.2. Внешние критерии
    Обычно ИМСОМ использует следующие два критерия:
    ?	критерий регулярности Kr ;
    ?	критерий несмещенности Ku .
    Оба критерия используют как обучающую, так и контроль-
ную выборку. Критерий регулярности отражает различие между 
моделью и контрольными данными, когда модель строится на 
обучающей выборке. Таким образом, этот критерий оценивает 
устойчивость модели к вариациям данных. Критерий несмещен-
ности отражает различие между двумя моделями - построенной 
по обучающей выборке и построенной по контрольной выборке. 
Таким образом, этот критерий оценивает независимость модели 
от данных.
    Могут быть предложены различные формы этих критериев. 
Выбранная форма зависит от конкретной проблемы. В нашем 
случае мы используем эти критерии в следующих формах:
    <object>
                                (3)
    <object>
Здесь T и C - это системы уравнений (2), используемые для обу-
чения и для контроля, соответственно; qi(T) и qi(C) являются "мо-
дельными" данными, т.е. значениями правых частей уравнений с 
параметрами, определенными по обучающей и контрольной вы-
борке, соответственно; qi являются "экспериментальными" дан-
ными, т.е. левыми частями уравнений; i - номер уравнения.
    Иногда одна модель может быть лучше другой по одному 
критерию и хуже по другому. Тогда используется комбиниро-
ванный критерий, т.е.:
    <object>K = ?Kr  + (1-?) Ku,                           (4) </object>
где ?-произвольный коэффициент, который отражает предпочте-
ния пользователя, <object>?є(0,1) </object>. В наших 
экспериментах с испанским языком мы брали 
<object>?=2/3</object>, т.е. мы рассматривали критерий регуляр-
ности как вдвое более важный, чем критерий несмещенности.
    3.3. Отбор примеров
    Эмпирическая формула, которая должна быть построена, рас-
сматривается как первое приближение. Позже полученная формула 
может подстраиваться под тексты из данной предметной области. 
Таким образом, примеры, отобранные для обучения и контроля, 
должны отражать только некоторые общие закономерности языка.
    Мы приняли следующие допущения: а) начальные общие час-
ти подобных слов распределены равномерно между самой корот-
кой и самой длинной подстроками; б) финальные части подобных 
слов также распределены равномерно между самой короткой и са-
мой длинной подстроками. Таким образом, мы взяли 50% пар слов 
с самыми короткими общими частями и 50% пар слов с самыми 
длинными общими частями. В каждой паре мы старались брать 
слова с короткими и длинными окончаниями. Мы не рассматри-
вали слова, содержащие менее 4 букв, и заменили буквы с ударе-
ниями на буквы без ударений (т.е., e ? e, a ? a и т.д.).
    Мы брали по 10 примеров как для обучения, так и для конт-
роля, что обеспечивало возможность вычислять до 4 параметров 
модели (кубический полином): число уравнений было в 2,5 раза 
больше числа переменных.
    В табл. 1 представлен набор пар подобных слов на испан-
ском языке, которые мы использовали для процедуры обучения.
    С этими примерами мы получили системы линейных урав-
нений, представленные в табл. 2. Подобная линейная система бы-
ла также построена и для кубической модели. Используя метод 
наименьших квадратов, мы определили 4 набора параметров (a, 
b1, ...) для каждой из упомянутых моделей для последующего их 
использования во внешних критериях.
Таблица 1. Примеры для процедуры обучения
<object>
№
Короткие слова

№
Длинные слова
1.
Celo
Celosamente

6.
Arrogante
Arrogancia
2.
Cazar
Cazador

7.
Institucional
Institucionalmente
3.
Arte
Artistico

8.
Multiplicados
Multiplicaciones
4.
Comer
Comida

9.
Descentralizados
Descentralizables
5.
Altura
 Altitud

10.
Caracteristica
Caracterizaremos
</object>
Таблица 2. Системы уравнений разной сложности
<object>
№
Модель 
порядка 0

№
Линейная
модель

№
Квадратичная
модель
1.
7/15  =  a

1.
7/15  = a +   4b1 

1.
7/15  = a +   4b1 +  16b2
2.
4/12  =  a

2.
4/12  = a +   4b1

2.
4/12  = a +   4b1 +  16b2
3.
7/13  =  a

3.
7/13  = a +   3b1

3.
7/13  = a +    3b1 +  9b2
4.
5/11  =  a

4.
5/11  = a +   3b1

4.
5/11  = a +   3b1  +  9b2
5.
7/13  =  a

5.
7/13  = a +   3b1

5.
7/13  = a +   3b1  +  9b2
6.
5/19  =  a

6.
5/19  = a +   7b1

6.
5/19  = a +   7b1 +  49b2
7.
13/31=  a

7.
13/31= a + 13b1

7.
13/31= a +  13b1 + 169b2
8.
9/29  =  a

8.
9/29 = a  + 10b1

8.
9/29  = a +   10b1 +100b2
9.
7/33  =  a

9.
7/33 = a  + 13b1

9.
7/33  = a +   13b1+ 169b2
10.
12/30 =  a

10.
12/30= a + 9b1

10.
12/30 = a +   9b1 +  81b2
    </object>
    Такие же линейные системы были построены для процедуры 
контроля. Решение каждой из этих систем методом наименьших 
квадратов позволило определить параметры моделей, которые за-
тем использовались во внешних критериях.
    3.4. Результаты экспериментов
    Было сделано 4 шага ИМСОМ, на каждом из которых опре-
делялись внешние критерии (3) и комбинированный критерий (4). 
Результаты представлены в следующей таблице: 
Таблица 3. Значения внешних критериев
<object>

Модель 
порядка 0
Линейная 
модель
Квадратичная 
модель
Кубическая
модель
Критерий Kr
0,26
0,19
0,18
0,23
Критерий Ku
0,09
0,11
0,12
0,18
Критерий K
0,20
0,16
0,16
0,21
    </object>
    Легко видеть, что наилучшей моделью оказалась линейная 
модель. На последнем шаге, чтобы увеличить точность модели, 
мы объединили обучающую и контрольную выборку примеров, 
получив, таким образом, систему из 20 уравнений с двумя неиз-
вестными. Итоговая эмпирическая формула для испанского языка 
выглядит так:
    <object> n/s ? 0,55 - 0,029 y                         (5) </object>
    Чтобы проверить чувствительность результатов применения 
формулы к изменению ее параметров, был взят набор статей раз-
ных авторов по популярной в Европе теме: "ипотека и кредиты". 
Набор состоял из 6 статей и содержал около 20 000 слов. Из этого 
набора случайным образом были выбраны участки текста и уда-
лены все стоп-слова и слова длиной менее 4 букв. Итоговый 
обобщенный текст содержал 536 слов. Так как мы приняли допу-
щение, что подобные слова имели различные финальные части, 
то было естественно упорядочить все слова по алфавиту и срав-
нивать между собой соседей. Конечно, это не лучший способ для 
группировки подобных слов, но он позволил упростить процеду-
ру ручной проверки.
    В экспериментах мы проверяли подобие смежных слов вруч-
ную и автоматически при различных комбинациях параметров 
формулы, которые менялись на ?10%. Качество оценивалось пол-
ной статистической ошибкой Perr=Pp+Pn и F-мерой F=2/[1/R+1/P]. 
Здесь Pp и Pn - вероятности статистической ошибки 1-го и 2-го 
типа, R и P полнота (recall) и точность (precision). Первая оценка 
обычно используется в математической статистике [Крамер Г. 
Математические методы статистики. М., 1976.], а вторая - при 
информационном поиске [Baeza-Yates R., Ribero-Neto B. Modern 
Information Retrieval. Addison Wesley, 1999]. Табл. 4 отражает результаты 
этого эксперимента.
Таблица 4. Проверка чувствительности формулы
<object>
535 проверок смежных слов = 165 подобных  +  370 не подобных
Параметры
 0,55,  
-0,029
 0,49, 
-0,029
 0,61, 
-0,029
 0,55,  
-0,026
 0,55, 
-0,032
Случаи подобия
Случаи различия
164
371
128
407
183
352
167
368
142
393
Ложная тревога 
Пропуск
22
23
8
45
34
16
23
21
14
37
Ошибка 1-го типа Pp
Ошибка 2-го типа Pn
5,9%
13,9%
2,2%
27,3%
9,2%
9,7%
6,2%
12,7%
3,8%
22,4%
Полнота  R
Точность P
86,1%
86,6%
72,7%
93,8%
90,3%
81,4%
87,3%
86,2%
77,6%
90,1%
Итоги 
Min Perr = 18,9%          Max F = 86,6%
    </object>
    Видно, что наилучшие результаты были получены при при-
менении эмпирической формулы <object> n/s ? 0,55 - 0,026 y. 
</object>
    Примеры пар слов, которые вызвали ошибки при обработке 
текста:
?	ошибка 1-го типа: ahora/ahorro (сейчас/экономия);
?	ошибка 2-го типа: invertido/inversores (вложенный/инвес-
торы);
?	ошибка 3-го типа: bancario/bancarrota (банковский/банк-
рот).
    3.5. Дискуссия
    Предложенный подход не использует морфологию языка и 
статистику распределения букв в словах языка. С этой точки зре-
ния он может рассматриваться как языковонезависимый. Однако 
при построении формулы неявно используются статистические 
свойства языка, отраженные в примерах. Именно поэтому подбор 
примеров следует поручать квалифицированным переводчикам, а 
не просто носителям языка.
    Полученная формула отражает следующее статистическое 
свойство языка, подтвержденное лингвистами: длина финальной 
части подобных слов в среднем одинакова как для длинных, так и 
для коротких слов. Действительно, из формулы (5) следует, что 
порог принятия гипотезы о подобии слов ниже для длинных слов, 
чем для коротких, при равном относительном числе несовпавших 
букв. Эта дискриминация длинных слов как раз и отражает ука-
занное выше свойство. Подобное имеет место также для англий-
ского языка, для которого эмпирическая формула подобия (пер-
вое приближение) выглядит так: <object> n/s < 0,55 - 0,032 y. 
</object>
    4. Построение частотных списков слов
    4.1. Основные алгоритмы
    Операция проверки подобия слов так, как она реализуется в 
настоящей статье, не является транзитивной. Имеется в виду, что 
из подобия двух слов третьему не следует, что первые два слова 
подобны между собой. Поэтому в алгоритмах построения частот-
ных списков слов результат существенно зависит от порядка срав-
нения. В настоящей работе рассматривается два алгоритма, отли-
чающиеся порядком сравнения, которые применяются к списку 
слов, упорядоченному по алфавиту:
?	алгоритм, где используется объединение смежных подоб-
ных слов;
?	алгоритм, где используется объединение как смежных, 
так и не смежных подобных слов.
    До начала работы каждого алгоритма текст или группа тек-
стов преобразуется в последовательность слов и все стоп-слова 
(включая короткие слова) удаляются из списка. С каждым словом 
связывается счетчик, устанавливаемый в состояние 1. Затем все 
слова упорядочиваются по алфавиту, одинаковые слова заменя-
ются одним словом, а их счетчики суммируются.
    Первый алгоритм состоит из следующих шагов.
1.	Начиная с первого слова списка, ищется слово, подобное 
следующему за ним.
?	Если такой пары слов не будет найдено, алгоритм за-
канчивает работу.
?	В противном случае два слова заменяются одним но-
вым - их общей начальной частью - со счетчиком, 
равным сумме счетчиков двух исходных слов.
2.	Процедура повторяется с текущей позиции.
Это алгоритм является однопроходным.
Второй алгоритм проверяет подобие каждого слова с каж-
дым. Он состоит из следующих шагов.
1.	Начиная с первого слова списка, ищется первое слово, по-
добное ему, среди всех слов с той же начальной буквой (не 
обязательно смежное).
?	В случае неуспеха процесс повторяется со второго 
слова.
?	В случае успеха, первое слово заменяется новым - их 
общей начальной частью - со счетчиком, равным сум-
ме счетчиков подобных слов. Затем второе слово из 
сравниваемой пары удаляется из списка. И, начиная с 
позиции удаленного слова, продолжается поиск слова, 
подобного новому первому среди слов с той же на-
чальной буквой.
2.	Процесс повторяется со второго слова скорректирован-
ного списка, и так далее.
Это алгоритм является многопроходным.
    Каждый из предложенных алгоритмов может быть реализо-
ван в двух вариантах:
?	с прямым ходом алгоритма, когда список обрабатыва-
ется от начала к концу;
?	с обратным ходом алгоритма, когда список обрабаты-
вается от конца к началу.
    Все реализации отличаются по своим возможностям объеди-
нения подобных слов.
    4.2. Результаты экспериментов
    Для экспериментов мы взяли тексты на испанском языке из 
коллекции документов, описанных в разделе 3.4. Из этих текстов 
был составлен новый текст, который не имел пересечений с теми 
текстами, которые использовались при настройке эмпирической 
формулы.
    Для такого объединенного текста был построен алфавитный 
список слов, из которого были удалены все стоп-слова, слова дли-
ной менее 4 букв и слова, которые не имели хотя бы одного подоб-
ного. Последняя процедура позволила сделать данный текст более 
репрезентативным с точки зрения проверки работы формулы подо-
бия слов. После этих преобразований список содержал 337 слов.
    В экспериментах мы использовали формулу с наилучшими 
параметрами из раздела 3.4.: <object> n/s ? 0,55 - 0,026 y. 
</object>
    В табл. 5 приведены результаты применения первого алго-
ритма в обоих вариантах его реализации, а в табл. 6 - результаты 
работы второго алгоритма.
Таблица 5. Качество работы 1-го алгоритма
<object>

Прямой ход
Обратный ход
Полнота  R
92,5%
95,4%
Точность P
89,4%
95,0%
F-мера
90,9%
95,2%
</object>
Таблица 6. Качество работы 2-го алгоритма
<object>

Прямой ход
Обратный ход
Полнота  R
94,2%
95,4%
Точность P
88,8%
95,0%
F-мера
91,7%
95,2%
    </object>
    Следует сказать, что объем проделанных экспериментов яв-
ляется достаточно ограниченным, так как их целью было проде-
монстрировать работоспособность предложенных алгоритмов, а 
не детальное исследование их свойств.
    4.3. Дискуссия
    Для обоих алгоритмов вариант реализации с обратным хо-
дом оказался предпочтительнее. Это следствие того обстоятель-
ства, что длина общей части подобных слов в упорядоченном по 
алфавиту списке слов в среднем растет. Поэтому при прямом хо-
де алгоритма формула может не обнаружить подобие слов, стоя-
щих вначале и конце группы подобных слов. Тогда обратный ход 
дает сильный компенсационный эффект к усечению общей части 
подобных слов, увеличивая вероятность их объединения.
    В целом оба алгоритма дали примерно одинаковые результаты. 
Требуются дополнительные эксперименты, чтобы доказать пре-
имущества каждого алгоритма. Разберем этот вопрос подробнее.
    Рассмотрим следующие два списка слов (примеры представ-
лены М. Портером):
1)	cat, ..., catalogue, cataplasm, catastrophe, catenary,......cats;
2)	bead, ..., beagle, beagling, ..., bear, ..., beast, beastliness, ..., beat.
    В первом списке выделенные слова - cat (кот) и cats (ко-
ты) - подобны. Во втором списке выделенные слова - bead (бу-
сина), bear (медведь) и beat (удар) - имеют различный смысл. 
Первый алгоритм не может объединить выделенные слова, поэто-
му в первом случае он работает неправильно, а во втором - пра-
вильно. Второй алгоритм объединяет выделенные слова в каждом 
списке, поэтому в первом случае он работает правильно, а во вто-
ром неправильно. Таким образом априори невозможно сказать, 
какой алгоритм будет работать лучше.
    Мы ожидаем, что в среднем 2-ой алгоритм будет давать бо-
лее точные результаты, чем 1-ый, для узкоспециализированных 
текстов: маловероятно, чтобы в одном тексте или в группе тема-
тических текстов одновременно появились столь разные слова, 
как bead, bear и beat. Но наше утверждение требует проверки.
    5. Выводы
    1.	В статье предложен формальный способ проверки подо-
бия слов, основанный на эмпирической формуле. Построение та-
кой формулы не требует никаких морфологических словарей и 
списков правил данного языка и может быть реализовано вруч-
ную или автоматически на основе небольшого числа примеров.
    2.	В статье предложены алгоритмы составления частотных 
списков слов, где используется эмпирическая формула. Работа 
алгоритмов продемонстрирована на реальном примере.
    3.	Алгоритмы составления частотных списков слов можно 
настроить на допустимый уровень ошибок незначительным изме-
нением параметров формулы.
    4.	Указанные обстоятельства делают предложенный подход 
полезным при работе со смешанным корпусом документов, кото-
рый содержит тексты на разных языках и разной тематической 
направленности. Для этого необходимо соответствующим обра-
зом менять параметры формулы.
    В будущем мы планируем построить ряд других эмпирических 
формул и сравнить их с построенной в настоящей работе. 
Необходимо провести сравнение эмпирических формул с работой 
стеммера Портера. Мы также планируем провести более детальные 
эксперименты с алгоритмами построения частотных списков слов.
    Мы благодарим проф. М. Портера за полезное обсуждение.
 
 
 
 
  
  
    
18
    
