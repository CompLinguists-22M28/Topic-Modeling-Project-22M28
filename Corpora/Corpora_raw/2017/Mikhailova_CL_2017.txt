П. Л. Гроховский, М. О. Михайлова
АВТОМАТИЗАЦИЯ СЕГМЕНТАЦИИ И ЧАСТЕРЕЧНОЙ РАЗМЕТКИ ТИБЕТСКОГО ТЕКСТА
В ходе работы над разметкой двух корпусов тибетских текстов (грант ФФЛИ № C-47 «Базовый корпус тибетского классического языка с русским переводом и лексической базой данных» и грант РФФИ № 13-06-00621 А «Пилотная версия электронного корпуса тибетских грамматических сочинений») возникла необходимость частично автоматизировать дальнейшую разметку. Разработка программных средств для предварительной автоматической разметки и сегментации тибетского текста осуществляется в рамках гранта РГНФ 16-0412016 «Программные средства автоматической обработки текста на современном тибетском языке (морфологический уровень)».
За стандарт разметки и сегментации принят формат корпуса грамматических сочинений. Это исключало использование разметки по словарю с последующим снятием неоднозначности по правилам, разработанной группой британских исследователей [Garret et al. 2014] в связи с различиями в принципах сегментации и наборе тегов. На 
пример, анализ данных имеющихся корпусов показывает, что части-цы, выделяемые ими как глагольные [Garrett 2015: 70], могут следовать и за токенами других типов. Поэтому на основании реальных данных было принято решение выделять такие частицы в отдельный класс, а не относить к глагольным аффиксам. Другое отличие — решение объединить в один токен глагольную основу и формообразующие аффиксы глагола, поскольку между ними ничего невозможно вставить, и обособить аффиксы падежей существительных в отдельные токены, поскольку падежный аффикс присоединяется справа к именной группе и может быть отделен от существительного определениями.
Изначально предполагалась только предварительная автоматическая разметка заранее сегментированного текста, поскольку автоматическая сегментация тибетского текста мало разработана. Инструменты для сегментации текстов на языках на базе латинской письменности, использующей пробел в качестве разделителя слов (например, nltk), не подходят для языка, не содержащего никакого графического разделителя слов, даже при уcловии разработки тибетского языкового пакета для них, что теоретически возможно, хотя очень трудоёмко.
Данный словарь возможно автоматически пополнять при добавлении новых размеченных текстов, он позволяет сортировать и искать данные по любому столбцу — например, увидеть подряд все токены одной части речи или все токены с одной леммой. За пределами данного проекта этот словарь может использоваться как словарь частот, лемм и частей речи, а также для решения комплексных задач, например, для поиска омонимов разных частей речи (совпадающее написание глагола и существительного) можно делать поиск по паре лемма+тег. Общий объём корпусов, по которым составлен словарь, 91825 токенов, в том числе 9112 уникальных токенов и 6111 лемм. Второй словарь, который используется для токенизации и разметки — словарь форм глаголов Нейтена Хилла [Hill 2010], объединяющий данные нескольких ранее созданных глагольных словарей.
Программные средства написаны на Python 2.7. Скрипт присуждает каждому токену лемму (леммы) и тег (теги) и записывает удобный для редактирования и проверки файл .csv, где каждая строка содержит один токен и его атрибуты. Для упрощения проверки разметки написаны инструменты для поиска, в том числе по любому атрибуту или их сочетанию и поиск контекста. Для сочетаемости с корпус-менеджерами написаны конвертеры табличного файла в вертикальный формат, xml или формат для nltk).
Так как частичная автоматизация обработки предполагает последующую ручную проверку, программа для разметки должна была выдавать все варианты разметки, найденные для данного токена в доступном материале, и оставлять неразмеченными все новые токены — при разметке только по изначальному словарю они составляли около 16 %. После успешного проведения эксперимента по автоматической сегментации основным режимом работы стало одновременное вы-полнение сегментации и разметки неподготовленного текста, так как это значительно сокращает работу лингвиста.
Сегментация построена следующим образом. Полностью обрабатывалась каждая строка входящего текстового файла по отдельности. Она разбивалась на графические слоги по разделителю слогов tsheg. Если после этого строка была слишком длинной (более 20 слогов) — использовался генератор подстрок, который разбивал длинную строку на фрагменты по знакам препинания (вертикальная черта shad и пробел) и передавал для разбора эти фрагменты по одному. Далее такой фрагмент разбивался на единицы, подходящие для поиска по словарю.
На этой стадии от слогов, содержащих только буквы тибетского алфа-вита, отделялись знаки препинания и цифры, обособлялись элементы, которые не отделяются от предыдущего слога графически, но являются отдельными морфемами: падежный аффикс 7, финитная частица о, вопросительная частица am и глагольный аффикс ang. Затем каждая строка в виде списка таких единиц передавалась в функцию, которая осуществляла по словарю поиск последовательностей максимальной длины. Это важно для тибетского языка, где почти все буквы могут формировать отдельный слог и слово.
После окончания сегментации и разметки скрипт выводит на печать общее количество токенов, количество размеченных токенов, количество неразмеченных токенов и время, затраченное на обработку. При наличии уже сегментированного текста возможно также отдельно использовать функцию разметки, без одновременной сегментации.
Анализ лакун и ошибок сегментации и разметки выявил следующие проблемы, послужившие основными направлениями доработки системы.
Во-первых, недостаточная полнота словаря. Этот недостаток исправляется по мере пополнения словаря, но лишь частично: с добавлением в словарь всего материала из корпуса текстов общего содержания, то есть, увеличении материала почти в три раза относительно изначального объёма, количество неразмеченных токенов сократилось всего на 5 %, до 11 %.
Во-вторых, многие ошибки относятся к открытым классам словоформ с очень большим варьированием (числительные, цифры, формы глаголов), образование которых которые можно описать правилами. Введение системы таких правил сократило количество неопознанных токенов до 7 %.
Эти правила используются на стадии поиска возможных комбинаций слогов в словаре. Использовались следующие правила:
•	Выделение форм глаголов: скрипт объединял глагольную основу (только основы, найденные в словаре с тегом “V”) с глагольными аффиксами из списка, и присуждал такой словоформе тег в зависимости от аффикса, что позволяет опознать и разметить даже те формы глаголов, которые не встретились в существующих корпусах и поэтому отсутствуют в словаре. Словарь аффиксов и тегов составлен по грамматическому корпусу.
•	Выделение чисел как непрерывных последовательностей латин-ских или тибетских цифр.
•	Выделение количественных числительных как единиц, состоящих только из слогов, которые образуют числительные, по списку возможных составляющих. Этот список взят из монографии [Beyer 1992: 221-226] и включает в себя как числа, так и специфические для тибетского языка разделители разрядов.
•	Выделение порядковых числительных как токенов, которые на конце имеют тибетский аффикс -pa, а вся предшествующая часть которых является количественным числительным.
•	Выделение сокращённых форм падежей: для двух падежей (терминатива и эргатива) после гласных используются сокращённые формы. Их особенность в том, что они сокращаются до одной буквы (‘r’ и ‘s’ соответственно) и встраиваются в предшествующий графический слог. Отделить в самостоятельные токены все ‘r’ и ‘s’ нельзя, так как таких случаев гораздо меньше, чем случаев, когда эти согласные являются финалью токена, поэтому при поиске последовательностей, оканчивающихся на эти согласные, также проверяется наличие в словаре этой последовательности без последней буквы. Если она есть, то выделяется два токена: найденная в словаре последовательность и падежный аффикс.
Система была протестирована на корпусе из 8 тибетских текстов научного стиля общим объемом 55 533 токена, 52 539 (94 %) токенов было размечено и 2994 (6 %) — нет. Точность (precision) и полнота (recall) совпадают в пределах округления. При тестировании только разметки на эталонной сегментации процент неразмеченных токенов составил 6,2 %, строгая точность 45 % (только один вариант разметки и он верный), нестрогая 92,3 % (наличие нескольких вариантов разметки, в том числе правильного), неправильно размечено 1,5 % токенов. Некоторая часть неразмеченных фрагментов оказалась результатами ошибок в текстах. При тестировании на тех же текстах после очистки от таких ошибок, количество неразмеченных токенов сократилось на 0,7 % (до 5,5 %), это примерно 240 токенов на весь корпус. При одновременном выполнении сегментации и разметки F-мера разметки при строгой оценке составляет 46 %, при нестрогой — 85 %, 7 % токенов не размечено, F-мера сегментации составила 88 %.
Что касается скорости разметки, то сегментация и разметка самого большого текста, который был сегментирован в ходе работы над этим проектом — 20 тысяч токенов в автоматически размеченном варианте — обрабатывался примерно две секунды. Работа только в режиме разметки (при уже сделанной человеком сегментации) требует меньше времени: сама разметка занимает 0.1 секунды для того же текста.