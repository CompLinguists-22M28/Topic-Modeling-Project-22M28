Ю. С. Масленникова, В. В. Бочкарев, В. Д. 
ВЕРОЯТНОСТНАЯ МОДЕЛЬ ДЛЯ ОЦЕНКИ ОБЪЕМА ЛЕКСИКОНА
1.	Введение
Несмотря на длительную историю изучения языков, мы до сих пор не знаем, даже приблизительно, сколько же слов содержит конкретный язык. Возьмем для примера английский язык. Наиболее полный из изданных до сих пор словарей английского — Oxford English Dictionary [OED Online 2017] — содержит в настоящее время более 600 000 слов. Однако, очевидно, что это не все слова языка. Например, они не содержат чрезвычайно редкие слова (встречающиеся реже чем 1 на миллиард словоупотреблений). Надежда на получение доcтаточно полного списка слов языка появилась после создания Google корпуса Google Books Ngram, содержащего более 500 млрд. слов английского языка. В [Michel et al. 2011] предпринята попытка оценить число слов по этому корпусу. Оценки получены только в трех точках. В 1900 г. язык содержал, по их оценке, 544 тыс. слов, в 1950 — 597 тыс., в 2000 — 1022 тыс. слов. В [Michel et al. 2011] приведены графики числа слов, полученные линейной экстраполяцией на остальные годы 20-го века. Подход, примененный в [Michel et al. 2011], не учитывает в полной мере очень редкие слова. Такие слова могут существовать в языке, но не появиться в некотором году, в силу ограниченного объема текстов в данный год. В настоящей работе предлагается математическая модель, позволяющая учесть этот фактор и более точно оценить реальное число слов, например, в ранние года. В основе метода лежат прогностические оценки «в прошлое» частоты употребления редких словоформ, которые потом используются для расчета более правдоподобных оценок вероятности появления словоформы в лексиконе с учетом объема корпуса.
1.	Метод и результаты
прошлое» отсчеты х^ (буква b означает слово «back» — «назад») могут быть записаны, как х£ = — Y3c=iakxt+k-
При поиске коэффициентов а|: данной модели следует учесть закон распределения значений исследуемого ряда. В нашем случае речь идет об оценке числа употреблений редких слов, а, следовательно, можно ожидать, что такой ряд распределен согласно закону Пуассона, который задается функцией вероятности:
Дисперсия и математическое ожидание случайной величины — числа употреблений словоформы, распределённой в соответствии с законом Пуассона, равны параметру распределения: M(X) = D(X) = Я. Наиболее точные оценки параметра Я можно получить с использованием метода максимального правдоподобия [Jackson 1989]. В этом случае максимизируется логарифмическая функция правдоподобия вида:
Аналогично можно построить также и нелинейную модель прогнозирования с использованием аппарат искусственных нейронных сетей с обучением по методу максимального правдоподобия. Более подробно данный подход представлен в статье [Maslennikova et al. 2014].
Алгоритмы прогнозирования были протестированы на временных рядах частот, полученных из корпуса Google Books Ngram по 20 тыс. английских словоформ, которые редко встречались в 1800-х годах. При использовании указанного выше подхода выигрыш в точности оценки частоты употребления оказывается тем больше, чем меньшую частоту имеет слово в анализируемый период. Для примера, при частоте 0.5 употреблений в год среднеквадратическое отклонение оценки уменьшается в 2 раза по сравнению с обычной оценкой по среднему значению эмпирической частоты. Прогнозирование частот употребления на разные горизонты времени показало, что с увеличением горизонта прогноза СКО увеличивается.
Для примера на рис. 1 показаны результаты прогнозирования «в прошлое» частоты употребления двух редких словоформ ‘shiftlessness’ и ‘tunnelling’ простой моделью линейного предсказания первого порядка (данная модель приводит к экспоненциальной зависимости частоты от времени). График представлен в логарифмическом масштабе по оси ординат.
На примере прогнозирования частот употребления всех 20 тысяч редких английских словоформ было показано, что ошибка прогнозирования в случае линейного и нейросетевого подходов распределена приблизительно по логнормальному закону (рис. 2). На рис. 2 показана плотность распределения натурального логарифма ошибки прогнозирования частоты употребления словоформ, а также ее аппроксимация методом ядерных оценок и нормальным законом распределения (т = 0,34, а = 1,86). Из рисунка видно качественное соответствие распределения ошибки логнормальному закону.
Знание закона распределения ошибок дает возможность при наличии прогноза на интересующий нас интервал времени оценить частоты употребления по критерию максимума апостериорной вероятности. Данный критерий дает существенное преимущество по сравнению с оценками по среднему значению эмпирической частоты.
Наличие хороших оценок частоты употребления словоформ для раннего периода дает возможность использовать эту информацию для уточнения действительного объема лексикона и скорости словообразования. Пусть в тот или иной год слово впервые фиксируется в корпусе. Это может означать, что ранее уже присутствовавшее в живом языке слово попало в корпус из-за увеличения его объема. С другой стороны, это может быть действительно новое слово. Экстраполируя частоту на предшествовавшие годы, мы можем рассчитать, какова вероятность того, что слово с такой частотой не будет зафиксировано в корпусе известного объема. Выполнив такие подсчеты для каждого слова, получаем возможность уточнить объем лексикона (и соответственно, скорость появления новых слов) для разных лет. Пусть ft это относительная частота употребления слова за определенный год t, Nt — объем корпуса в словах за этот год. Тогда указанная вероятность может быть оценена, как Р(Х = 0) = Пг(1 — ft)Nt То есть, по сути, решается задача проверки статистической гипотезы о том, что словоформа присутствовало в языке и ранее того момента, когда в первый раз было зафиксировано в корпусе. Например, для словоформы ‘shiftlessness’ вероятность того, что ранее 1800 г. словоформа не попала в базу из-за малого объема корпуса составила 0,75, а для словоформы ‘tunnelling’ — 0,56. Видно, что данная вероятность достаточно высокая.
Эффективность предложенного метода была проверена посредством статистического моделирования. Для этого были отобраны случайным образом ряд слов и построены временные ряды их частот. Далее значения частот занижались в нужное число раз, и генерировались случайные временные ряды с пуассоновским распределением, соответствующие данной временной зависимости частоты. После этого выполнялась оценка частоты для ранних периодов и определялась ошибка. Таким образом были получены значения среднеквадратической ошибки оценки при различном уровне средней частоты и определен выигрыш, получаемый по сравнению со стандартным подходом.
3.	Заключение
Используя оценки частот для 20 тысяч редких словоформ английского языка, был уточнен объем лексикона для разных лет и, соответственно, скорость образования новых слов. В рамках рассмотренной вероятностной модели было показано, что в более раннее опубликованных работах скорость появления новых слов завышена как минимум в 2 раза.