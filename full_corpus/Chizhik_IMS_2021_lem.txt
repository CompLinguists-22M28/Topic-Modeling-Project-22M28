использование метод тематический моделирование для оценка степень влияние сми на общественный настроение а.в. чижик санкт-петербургский государственный университет 1. постановка проблема классический задача в область обработка естественный язык являться тематический моделирование, цель который – поиск скрытый структура дать (создание модель коллекция текстовый документов). такой модель определять набор тем, который содержаться в серия документов, что позволять рассортировать этот документ по различный тематический категориям. поскольку количество тем неизвестно, то этот задача относиться к пул задача обучение без учитель (unsupervised learning): на вход присутствовать немаркированный набор данных, алгоритм должный самостоятельно провести они логический классификацию. такой образом, тематический моделирование очень похоже на проблема кластеризация данных. с помощь моделирование тем, по сути, происходить группировка текстов, при это кластер приобретать интерпретация как тематический категории. основный отличие состоять в том, что, оказываться в категория тематический моделирования, приходиться перейти от более традиционный евклид векторный пространство к некоторый абстрактный пространство слов. метод тематический моделирование можно разделить на два основный группа – алгебраический и вероятностные. латентно-семантический анализ (lsa) относиться к алгебраический методам, а среди вероятностный наиболее популярный являться латентный размещение дирихнуть (lda). так как lsa и lda основать на очень разный математический процедурах, то, очевидно, что в зависимость от тип входной текстовый дать они быть иметь разный степень успеха. при это алгоритм использование они в рамка прикладной задача мочь быть достаточно схож. 2. набор дать для эксперимент быть собрать коллекция новостной анонсов, опубликовать на официальный страница интернет-издание «лента.ру» в социальный сеть «вконтакте» за период c 1 январь 2020 г. по 20 март 2021 г. общий количество слово в исследовать коллекция – 1766152 слов, средний количество слово в один анонс – 11. в необработанный вид новостной анонс являться серия текстовый строк, сопровождать дата публикация (такж быть собрать технический информация о опубликовать постах, однако в это исследование она не являться значимой). ниже привести фрагмент сформировать датасет (рис. 1). так как прикладной цель исследование быть выявление корреляция между новостной повестка в сми и формироваться общественный мнением, то быть собранный ещё один набор дать за тот же временной период, который раскрывать присутствовать в социальный сеть обсуждение обычный человек на предложить темы. источник послужить публичный telegram-чат новостной канал mash – «масх», в который участник на фон официальный новость обсуждать текущий событие в мире, высказывать свой позиция (выводы), а также дополнительно освещать тема (факты). этот датасет включить в себя 56171 слов, средний количество слово в один сообщение - 13. формат коллекция – текст и дата он публикация (то есть, аналогичный описать выше). как правило, тематический моделирование предполагать достаточно длинный текстовый объект в качество исследовать единица (например, полный текст статьи), это связать с тем, что больший количество слово в документ помогать чёткий очертить потенциальный тему, а также составить объёмный тематический словарь. однако специфика новостной анонс позволять ожидать надёжный ядро семантический содержание за счёт лаконичность дискурса, свойственный этот тип журналистский контента. также необходимо отметить, что общий объём собранный запись обеспечить достаточный глубина анализ данных. 3. построение векторный представление текст для тот чтобы текст быть возможно использовать в качество входной дать в любой алгоритме, необходимо преобразовать языковой сущность (слова, предложения, параграф или текст в он полный объеме) в набор число (числовой вектор). оперировать такой векторами, в частности, становиться легко представить в геометрический пространство близость слово друг к другу. такой вектор называться word embedding. в случай данный исследование процесс векторизация подвергаться по отдельность каждый новостной анонс (и далее – каждый сообщение пользователь из второй набор данных). самый простой подход к векторизация модель «мешок слов» (bag-of-words model, bow), в рамка который пренебрегаться порядок слов, составляться словарь присутствовать слов, такой образом, каждый слово становиться возможный превратить в вектор по длина такой словаря. такой вектор показывает, сколько раз каждый слово из словарь встречаться в конкретный документе. этот способ векторизация называться one-hot-encoding, и, в целом, давать необходимый количество операция над закодировать текст для того, чтобы он можно быть успешно проанализировать. так, например, можно сложить вектор весь слово в предложение и получить вектор суммы. также такой набор вектор давать информация о том, насколько часто в предложение встречаться разный слова. к тот же вектор предложение можно сравнивать между собой. распространить альтернатива это метод являться использование статистический мера tf-idf, который вычислять относительный частота слово в документ по сравнение с весь корпусом, помогать такой образ оценить важность каждый слово внутри конкретный документа. данный метод полезный при анализ коллекция неравномерный по длина текст в качество противодействие больший значениям, который более длинный документ иметь бы по сравнение с короткими, если бы использоваться необработанный подсчеты. однако исследовать набор дать обладать два важный характеристиками, во-первых, в немой присутствовать текст примерно одинаковый длинны, во-вторых, новостной анонс являться короткий текстами, поэтому использование метод tf-idf, скорее всего, привести не к улучшению, а к снижение качество векторизация в рамка дальнейший задача тематический моделирования. поэтому для это исследование быть выбрать наивный bow подход, который в конечный итог принести терм-документный матрица (document-term matrix), где каждый строка соответствовать новостной анонсу, а каждый столбец – отдельный слову. отметим, что при кодирование текстовый информация из набор дать быть отсечь стоп-слово (например, предлог и союзы) с цель обеспечение больший репрезентативности. 4. обзор модель lsa и lda скрытый семантический анализ (lsa) быть предложный для задача тематический моделирование в 2004 год [1].ть основа метод лежать идея о том, что слово быть встречаться в похожий часть текста, если они иметь одинаковый значение. на вход в lsa модель поступать матрица, состоять из m документ и n слово (создать с применение ранее описать методов, или любой другой способ векторизация текстовый данных). затем происходить процедура факторизация матрицы: алгоритм раскладывать матрица по сингулярный значениям, благодаря что получаться три новый матрица (на рис.2 отобразить быть процесс разложения), линейный комбинация который являться достаточно точный приближение к исходный матрице. основный идея заключаться в том, что матрица (topic matrix), получиться при перемножение новый ортогональных, который содержать только k первый линейно независимый компонент исходный матрицы, отражать структура зависимостей, который латентно присутствовать в исходный матрице. каждый из n строка этот матрица представлять себя документ, а каждый из первый k столбцы соответствовать теме. тогда (i, j)-тая запись мочь считаться мера присутствие тема j в документ i. чтобы отсортировать документ по тематический категории, достаточно узнать больший значение каждый строчка (argmax), который быть соответствовать наиболее широко представить теме. отметим, что количество тематический категория (параметр k), на который алгоритм быть делить тексты, являться задавать параметром. латентный размещение дирихнуть (lda) быть представить в 2003 год [2]. как генеративный вероятностный модель для коллекция дискретный данных. важный идейный момент lda заключаться в том, что вероятностный модель удобно понимать и представлять в вид порождать процесс (generative processes), то есть последовательно описывать, как порождаться единица данных, а имитя каждый слово в документ (указывать вероятностный распределения). основа метод – предположение о том, что в каждый документ смешать разный темы, а в каждый тема – присутствовать определённый распределение слов. интуитивно прочитываться два уровень агрегирования: 1) распределение по категория (к примеру, новость о экономике, политический новость и т.п.), 2) распределение слово внутри категория (например, «деньги» и «акции» актуальный текст о экономика и финансах). поэтому документ рассматриваться как распределение вероятность по скрытый темам, а этот тема - как распределение вероятность по словам. при это существовать большой количество слов, который появляться в текст любой тематика с одинаковый вероятностью. поэтому удаление стопслово и для это метод – важный шаг реализация алгоритма. теоретический обоснование lda полагаться на использование понятие взаимозаменяемость (теорема де финетти [3]), использовать который можно получить внутридокументный статистический структура через смешанный распределение. итак, метод предполагает, что процесс порождение каждый слово состоять в том, чтобы сначала выбрать тема по распределению, соответствовать документу, а затем выбрать слово из распределения, соответствовать этот теме. то есть, чтобы отсортировать новостной анонс по тематический кластерам, lda обращаться к априорный значение распределение дирихле, использовать вариационный байесовский метод для вывод скрытый параметр распределения, который затем характеризовать различный темы. как и в случай с lsa, количество тем являться гиперпараметраметром, который задаваться модель на входе. результат работа алгоритм представляться в форма матрицы, но каждый из строка теперь представлять себя распределение вероятностей, определённый по тема для каждый документа. поэтому (i,j)-тот значение этот тематический матрица мочь интерпретироваться как вероятность того, что заголовок i принадлежать тема j (точнее, как доля слово в заголовке, относиться к тема j). для получение оценочный категория тема каждый новостной анонса, необходимо вычислить больший значение каждый строчки. 5. описание эксперимент текстовый дать быть предобработать в следующий последовательности: разбиение текст на токены; удаление спецсимволов, ссылка и пунктуации; удаление стопа-слово и лемматизация токенов. далее текст быть векторизовать при помощь countvectorizer (библиотека scikit-learn), который возвращать закодировать вектор с длина весь словарь (поэтому вектор разреженные) и информацией, сколько раз каждый слово появиться в документе. так возникать терм-матрица, который быть подаваться на вход в оба алгоритм тематический моделирования. первый этап анализ стать выявление наиболее частотный слово в набор дать (без учёт стоп-слов), что дать возможность оценить словарь исходный дать (рис. 3). получиться диаграмма показывает, что, во-первых, провести предобработка текстовый дать оказаться достаточно, так как наиболее часто встречаться слово выглядеть интерпретируемо, во-вторых, интуитивно прочитываться несколько тем. 5.1. латентно-семантический анализ модель lsa реализовываться с помощь truncatedsvd (библиотека scikit-learn). количество искомый тем быть выбрать эмпирически – 20 (этый же число быть использоваться и для модель lda). взять argmax для каждый новостной анонс в получиться матрице, быть получить и отсортировать прогноз тем для весь объект в выборке. в каждый выделиться тематический топика быть найти наиболее часто встречаться слово (длить более лёгкий дальнейший интерпретации) – рис.4. на гистограмма видно, что модель lsa в целое определить некоторый темы, который интуитивно очерчиваться и благодаря общий анализ частота слов. при это распределение тем неравномерно, что свидетельствовать о том, что один тема более распространены, чем другие, в новостной репортажах. далее получить вектор быть преобразовать с использование техника нелинейный снижение размерность t-sne [4] для они отображение в двухмерный пространство. такой образом, 20-мерный тематический вектор быть сжатый в 2-мерный представления, чтобы выделить кластер (рис. 5). хотя получить выше тематический категория казаться в целое согласованными, диаграмма рассеяние показывает, что разделение между этот категория условное, есть участки, где кластер накладываться друг на друга. к такой результат в частность мочь привести то, что один и тот же термин мочь быть одинаково важный для несколько тем одновременно. 5.2. латентно-семантический анализ lda также реализовать с использование библиотека scikit-learn (latentdirichletallocation класс). также как описать выше для lsa по итог работа алгоритм быть вычислить argmax каждый запись в матрице, чтобы получить прогнозировать категория тема для каждый новостной анонса. затем этот тематический категория быть охарактеризовать по наиболее часто использовать словам, что проиллюстрировать на гистограмма (рис. 6). получиться результат отличаться от результатов, получить с помощь lsa: выделить тема более последовательны, к тот же распределение тем выглядеть более убедительно. вероятно, это следствие вариационный алгоритм байеса, который начинаться с равный априорный значение для весь категория и только постепенно обновлять они по мера прохождение через набор данных. для интерпретировать сравнение lda с lsa получить с использование это метод тематический матрица также быть спроецировать в двухмерный пространство (рис. 7). отображение кластер в двумерный пространство чётко показывает, что lda сработать для анализировать дать гораздо лучше: тематический кластер чётко разделить между собой, к тот же каждый тема отсортировать в почти непрерывный область (инверсивный картина наблюдаться на рис. 5). в качество завершать этап исследование быть решить выделить 4 наиболее популярный темы, детектировать алгоритм lda, чтобы обратиться к второй набор дать с цель выявление динамика обсуждение в telegram-канал тем, который настолько активно обсуждаться в сми (рис. 8). поиск проводиться по ключевой словам, выделить на предыдущий этапе. вывод скрытый семантический анализ (lsa) и скрытый распределение дирихнуть (lda) использоваться для определение присутствовать в новостной анонс тем. модель lsa не удаться добиться большой разделение между выделить кластерами, хотя выделить тема выглядеть достаточно интерпретируемыми, если ориентироваться на наиболее частотный слова. в то же время алгоритм lda продемонстрировать большой потенциал для подобный исследований, добиться хороший разделение между темами. для дальнейший оптимизация lda-модель видеться целесообразный выбор количество тематический группа производить путём оптимизация показатель качества. например, использовать показатель согласованности, который измерять семантический сходство между наиболее часто встречаться слово в теме. максимально увеличить показатель согласованности, удаться добиться ещё более хороший качество реализовать модели. получить график активность пользователь социальный сеть показывает, что сми оказывать влияние на динамика обсуждение тот или иной тема (на рис. 8 видный пик максимальный внимание и больший апатия к каждый из выделить тем, который совпадать с время появление новый релевантный информационный поводов), однако при это наблюдаться способность индивид к самостоятельный оценка актуальность освещать средство массовый информация повестка день (этый отчётливо видно по общий распределение количество обсуждение каждый из тем).