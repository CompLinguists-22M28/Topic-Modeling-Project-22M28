тематический моделирование русскоязычный текст с опора на лемма и лексический конструкция а.г. седова, о.а. митрофанов санкт-петербургский государственный университет, 1. введение в настоящий время в корпусный лингвистика активно развиваться направление вероятностный тематический моделирования, опираться на статистический обработка текстов. тема (topic, latent topic), или скрытый паттерн (hidden pattern) — это дискретный вероятностный распределение в пространство слово задать словарь [1]. формально говоря, тема являться результат би-кластеризации, то есть одновременный кластеризация и слов, и документ по они семантический близости. тематический моделирование при это называться восстановление вероятностный распределение весь тем в тексте, рассматривать как случайный независимый выборка слово («мешок слов»), породить некоторый темами. при это достаточно небольшой число тем мочь породить документ, состоять из большой количество слов. порядок тем при различный запуск алгоритм мочь варьироваться, что обусловить свойство неупорядоченности, или перестановочность (exchangeability) тем. тематический модель описывать связь между слово и темами, документ и тема с помощь смесь дискретный распределений. такой образом, тематический модель выступать как средство обобщение и систематизация информация из больший текстовый коллекция и позволять выявить скрытый структура и неявный зависимость в данных. тематический моделирование широко применяться для решение задача информационный поиск и машинный перевода, для кластеризация изображений, распознавание объект и рукописный текста, определение эмоциональный окраска текстов, выявление и анализ различный временной трендов, осуществление автоматический аннотирование и индексирование документ [2]. один из наиболее распространить метод построение тематический модель являться метод латентный размещение дирихнуть (latent dirichlet allocation, lda) [3]. данный алгоритм опираться на априорный распределение дирихнуть и использовать в свой работа модель «мешок слов» (bag-of-words) — модель для анализ текстов, который учитывать только частота слов, но не они порядок; дать модель хорошо подходить для многий метод тематический моделирования, поскольку она позволять обнаруживать неявный взаимосвязь между слово с учёт полисемии. метод lda осуществлять мягкий кластеризация и предполагает, что каждый слово в документ породить некоторый латентный темой, определяться вероятностный распределение на множество весь слово текста. в наш исследование мы пользоваться именно это алгоритмом. на вход практически любой тематический модель поступать корпус текстов, каждый из который являться отдельный документом. результат работа модель являться список тем, выявить в корпус и представить список первых, наиболее характерный n-слово для каждый рассматривать темы. в базовый алгоритм тематический моделирование тема представить исключительно униграмм (например, «система частота время измерение объект скорость дальность метод параметр обработка», «свойство расстояние отражённый излучать отражение радиолокационный земля объект мощность отражать»). в первый очередь это происходить благодаря использование модель «мешок слов», не учитывать линейный зависимость между слово внутри предложения. зачастую это влечь за себя ухудшение точность и повышать сложность содержательный интерпретация выделять тем, особенно в случай некомпозиционный словосочетаний, значение который не сводиться к сумма значение входящий в они слов: например, «железный дорога» не сводиться к значение слово «железная» и «дорога» соответственно [4]. такой образом, добавление в тема расширение тем за счёт n-грамм представлять себя актуальный исследовательский задачу. в последний время быть предложить несколько подход к решение дать проблема [5, 6]. однако многие из они снижать качество модель или же излишний усложнять её [4]. в дать работа быть предпринять попытка предложить новый метод, который бы действительно упрощать интерпретация тем и повышать они точность, оставаться при это понятный и простой для реализации. 2. алгоритм добавление биграмм в тематический модель 2.1. возможный решение задача в зависимость от использовать алгоритм предлагать для решение дать задача алгоритм делиться на два группы: ниже привести пример оформление списка: − алгоритмы, представлять себя унифицировать тематический модель, в рамка который словосочетание выделяться в текст одновременно с темами; алгоритмы, выделять многословный выражение на этап предобработка текста. большинство существующий алгоритм относиться к первый группе. один из они является, например, биграммный тематический модель (bigram topic model). дать модель являться иерархический порождающей, и при её работа в качество − основополагающий использоваться предположение о зависимость появление слово зависеть исключительно от интересовать мы словом: слово стоящий , где { } – гиперпараметр модели, непосредственно перед , — частотность слово , — частотность словосочетание [5]. однако недостаток данный алгоритм можно считать то, что он позволять выделять в документ только темы, состоять исключительно из биграмм, что мочь навредить точность и полнота тем. другой пример тематический модели, включать в себя выделение словосочетание в текст одновременно с темами, являться скрытый тематический марковский модель с применение латентный размещение дирихнуть (hidden markov model with latent dirichlet allocation, hmm-lda). в дать модель строиться совместный описание синтаксис и семантика текст с помощь разбиение каждый предложение на функциональный слова, который порождаться с помощь скрытый марковский модели, (таким образом, описываться локальный закономерности), и на термины, генерировать тематический модель lda (так даваться глобальный тематический описание документа) [7]. модель состоять из последовательность переменный-слово тематический переменный бинарный классификация указывающих, образовать ли данный слово и предыдущий словосочетание. значение слово выбирается, основываться на предыдущий , исходить из распределение образовать словосочетание и слово и последовательность . если , то слово и анализироваться в семантический аспекте, т. е. на основание тематический распределение : . если же , то слово порождаться из распределение : . несомненный достоинство унифицировать тематический модель являться они логический теоретический обоснование. однако к они минус можно отнести большой количество параметров, нуждаться в настройке. например, число параметр у биграммный тематический модель равно w2t, в то время как у базовый модель lda — wt, где w — размер словарь (т.е. число уникальный слово и словосочетание корпуса), t — число выделить тем. один из наиболее распространить алгоритмов, относиться к второй типу, то есть позволять выделить биграмм на этап предобработка текста, являться алгоритм, предложить в работа [8]. в данный алгоритм коллокация выявляться на этап предварительный обработка текста, упорядочиваться в соответствие с ассоциативный мера t-score: где tf(xy) — частотность словосочетание xy, tf(x) и tf(y) — частотность слово x и y соответственно, |w| - число различный слово в коллекции. далее наиболее удачный получить словосочетание объединяться в один токен и добавляться в корпус, заменять униграммы. такой образом, в процесс собственно построение тематический модель (в данный случае, lda) и построение модель «мешок слов», они рассматриваться наряду с другой униграмм как токены. другой алгоритм такой тип являться алгоритм plsa-sim и plsa-iter, предложить в [9]. оба дать алгоритм являться усовершенствовать версия один из базовый модель plsa — вероятностный латентный семантический анализа, основать на введение слой скрытый переменный для описание тематика документ из корпус текст [10]. – и также использовать в свой работа модель мешок слов. идея, который лечь в основа алгоритм plsa-sim, следующая: в любой текст существовать большой количество слово и коллокаций, семантически и лексичёска близких: например, бюджетный, бюджетный расходы, бюджетный средства, бюджетный доход [4]. в рамка данный алгоритм при выделение тем подобный словосочетание относиться к один теме. если же подобный слово и словосочетание никогда не встречаться в рамка один документа, то для они выполняться стандартный алгоритм plsa. дальнейший усовершенствование алгоритм plsa-sim являться итеративный алгоритм plsa-iter, в рамка который рассматриваться самый частотный униграммы, представлять темы, и из они составляться биграммы. в качество пример автор приводиться биграмм ценный бумага, который мочь быть составлена, если в некий тема среди первый n слово оказаться униграмм ценный и бумага. в [4] рассматриваться первый 10 униграмм, из который далее образоваться биграмм и добавляться в тематический модели. помимо этого, в дать модели, как и в предыдущей, учитываться частеречной принадлежность слов: в выделение тем участвовать только существительные, прилагательные, глагол и наречия, а в формирование биграмм для русский язык следующий коллокации: существительное + существительное в родительный падеже, прилагательное + существительное. подход, предполагать предварительный выделение биграмм в текстовый коллекциях, возможно, не иметь такой изящный теоретический обоснования, однако позволять строить алгоритмы, являться гораздо более простой в применении. в первый очередь это достигаться за счёт того, что количество настраивать параметр в дать модель равный они количество в исходный модель (как правило, lda или plsa). недостаток данный подход можно назвать повышение перплексии, что вести к ухудшение обобщать способность выявить модели. предлагать мы метод можно отнести к второй типу, поскольку он предполагать автоматический выделение биграмм на этап предварительный обработка текста, а затем уже они последующий добавление в тематический модели. 2.2. использовать корпусный дать для проведение эксперимент быть выбрать два корпус русскоязычный текстов: − корпус специальный текст по радиоэлектронике, ракетостроение и техника [11]; общий объём корпуса: 526 648 словоформ; − корпус русскоязычный специальный текст на лингвистический тематика из лингвистический энциклопедический словарь (лэс) под редакция в.н.ярцев и энциклопедия «кругосвет» [12]; общий объём корпуса: 1 333 546 словоформ. причина выбор текст именно научный стиль несколько. во-первых, специфический черта подобный текст являться а) значительный лаконичность, однозначность, б) малый экспрессивность, метафоричность и синонимия, что несомненно позволять упростить построение и интерпретация тематический моделей, повысить они точность и снизить доля шума. во-вторых, они характерный черта являться повышенный содержание терминов. термин (в рамка данный исследования: отдельный слово и двухсловный сочетания) всегда существовать в рамка терминосистемы, обладать фиксировать терминологический значение и, как правило, обозначать какой-то предмет, явление или, в крайний случае, процесс. с один стороны, это облегчать процесс тематический моделирования, поскольку тематика документ из специальный корпус текст предсказуема. с другой стороны, регулярность и воспроизводимость термин в рамка один текст упрощать выделение биграмм. также не стоить упускать из вид то, что, как правило, термин — это номинативный лексика, терминосочетание формироваться из существительных, глагол и прилагательных, и коллокат этот часть речь составлять наиболее частотный биграммы. на этап предварительный обработка текст из они удаляться нетекстовый символ и сокращение (в рамка данный эксперимент быть принять решение о удаление весь слов, длина который составлять менее 3 символов). помимо этого, из текст исключаться слова, входящий в список стопа-слово на основа словарь служебный слово и оборот нкря, а также 98 наиболее частотный глагол и отвлечь существительное (например, использовать, позволять, наличие, отсутствие и так далее). далее быть произвести лемматизация текст и автоматический разрешение морфологический неоднозначность с помощь морфологический анализатор русский язык mystem 3.0 [13]. после прохождение этап предварительный обработка текст объём корпус оказаться следующими: − корпус текст по радиоэлектронике, ракетостроение и технике: 216 613 леммы; − корпус текст на лингвистический тематику: 1 246 590 лемм. 2.3. описание алгоритм на первый этап работа алгоритм в исследовать корпус быть выявить биграмм с использование модуль phrases, входящий в состав библиотека gensim1. данный модуль, использовать модель «мешок слов», автоматически определять наиболее часто встречаться в документ многословный словосочетания. параметр, отвечать за принятие решение о формирование биграмм (threshold) основать на совместный встречаемость слов. слово а и b считаться биграммой, если: где n – общий размер словаря. в наш эксперимент в качество документ рассматриваться предложение из корпуса, поскольку больший интерес вызывать совместный встречаемость слово именно в предел синтагм. обучение модуль проводиться непосредственно на корпус текстов, с который вестись далее работа. технически алгоритм быть реализовать на язык программирование python. в процесс работа алгоритм в корпус текст быть образовать 14 542 биграмм для корпус текст по радиоэлектронике, ракетостроение и техника и 187 008 биграмм для корпус текст на лингвистический тематику. соотношение же с общий объём корпус составляет: 13,4% для корпус текст по радиоэлектронике, ракетостроение и техника и 30.0% для корпус текст на лингвистический тематику. возможный причина бόльший количество выделить биграмм в второй корпус несколько: во-первых, роль играть объём корпуса, поскольку от он напрямую зависеть параметр threshold: при увеличение объём число биграмм также увеличивается. во-вторых, стоить отметить разница в лексический специфика собранный корпусов. корпус текст на лингвистический тематика более однородный; лексический наполнение входящий в он текст более однообразно, наблюдаться бόльшея количество высокочастотный терминов. напротив, корпус по радиоэлектронике, ракетостроение и техника представлять себя тематически более разнородный выборка текст и изобиловать низкочастотный терминами. общий частота употребление термин в корпус снижаться за счёт лексический разнообразия, и поэтому малый количество униграмм проходить порог формирование биграммы. помимо этого, в соответствие с установленный мы параметр униграммы, встречаться в текст менее два раз, вовсе не рассматриваться при выделение биграмм; поэтому, вероятно, некоторый количество потенциальный биграмм, который быть образовать из низкочастотный терминов, не попасть в конечный список. пример состав корпус до предварительный обработки, после первичный обработки, а также после работа алгоритм по выделение биграмм привести в табл.1 и табл. 2. …переть qys>\ ошибка х — хо линейно связать с величина z'(хо), который мочь считаться гауссовский в сила нормальность z(x) в рассматривать условиях. как выясняется, разработать в 4.7 методика расчёт потенциальный точности, т. е. дисперсия омп, оказываться удовлетворительный только при условии, что превышение сигнал над шум настолько велико, что наблюдатель вправе полагать разброс x относительно хо полностью укладываться в предел линейный участок производный фн, сместить в точка х = х0. для это прежде весь необходимо, чтобы побочный (шумовые) выброс на 4.7, в не превосходить основный пика, обусловить фн сигнала… считаться линейно условие гауссовский величина связанный сила выясняться расчёт точность потенциальный методика удовлетворительный точка полностью предел вправе шум производный дисперсия условие превышение укладываться смещать разброс наблюдатель настолько сигнал линейный участок омп большой побочный шумовой выброс необходимо обусловливать пик сигнал основный превосходить линейно условие гауссовский_величина связанный сила выясняться расчет_точность потенциальный методика удовлетворительный точка полностью предел вправе шум производная_дисперсия условие_превышение укладываться смещать разброс наблюдатель настолько_сигнал линейный участок омп большой побочный_выброс шумовой необходимо обусловливать пик сигнал основный превосходить анализировать пример из корпуса, можно заметить, что в результат работа алгоритм быть образовать несколько несомненно правильный биграмм, хоть и не стоящий в данный отрывок в непосредственный близости: например, гауссовский_величина, побочный_выброс, расчет_точность и норма_произносительный, норма_орфоэпический, национальный_язык, норма_складываться. остальной же обобщённый униграмм в данный отрывок как таковой биграмм не являются. …кодифицированный норма часто отставать от реально сложившейся. орфоэпия складываться одновременно с формирование национальный языка, когда расширяться сфера действие устный речи, развиваться новый форма публичный речи. в разный национальный язык процесс становление орфоэпический норма проходить по-разному. орфоэпический норма мочь пройти несколько этапов, прежде чем стать норма национальный языка… отрывок корпус до предварительный обработки, перед запуск алгоритм по выделение биграмм отрывок корпус после работа алгоритм по выделение биграмм кодифицированный часто отставать реально норма складываться речь язык новый устный формирование национальный публичный одновременно действие развиваться форма складываться сфера орфоэпия расширяться язык разный национальный становление норма орфоэпический проходить разный процесс язык национальный норма орфоэпический проходить этап русский особенность кодифицированный часто_отставать реально норма_складываться речь язык новый устный_формирование национальный_публичный одновременно действие развиваться форма складываться сфера_орфоэпия расширяться язык разный национальный становление норма_орфоэпический проходить разный процесс язык национальный норма_орфоэпический проходить этап русский особенность на второй этап работа алгоритм быть построить тематический модель экспериментальный корпуса. как уже быть упомянуть ранее, мы быть принять решение использовать для это вероятностный тематический модель латентный размещение дирихнуть [3], включить в пакет для анализ дать scikit-learn2. эмпирический путём быть установить параметры, позволять выделить тема наиболее точно: количество итерация алгоритм — 200, количество тем — 20, количество слов, представлять каждый тема — 10 первый слов. последний параметр неслучайно быть выбрать именно таким: исследование показали, что именно 10 первый слово содержать в себя 30% информация о теме, распределённый в другой словах, что являться достаточный для достаточно полный представление тема [14]. также стоить отметить, что при построение тематический модель не учитываться слова, встретиться менее, чем в два документах, а также высокочастотный слово — в данный случае, содержаться более чем в 80% документов. на третий этап получить тема быть заново обработать с помощь модуль phrases, что позволить выделить ещё некоторый количество биграмм. получить конечный результат представить в табл. 3 и табл. 4. проанализировать получить результаты, можно заметить, что большинство выделить биграмм действительно образовать логичный словосочетания, такой как: линейный пространство, ширина спектра, случайный величина, преобразование фурье, суммарный канал, радиолокационный цель, кодовый слово, проверка гипотезы. остальной же выделить биграмм вполне объяснимы: например, биграмм потребитель_исз быть выделен, вероятно, вследствие того, что слово исз (искусственный спутник земли) и потребитель (в значение исследователь, наблюдатель) часто встречаться в такой схожий контекстах, как расстояние между потребитель и исз, скорость исз относительно потребитель и так далее. также слово обзор и рлс (радиолокационный станции), хоть и не встречаться в текст стоящий рядом, в многий контекст встречаться в непосредственный близости: например, рлс дальний обзора, рлс ближний обзора, обзорный рлс и т.п. результаты, получить на второй корпус текстов, отличаться от получить на первый в основный тем, что в тема выделиться значительно маленький биграмм: влияние_оказывать и словарный_статья, и оба дать биграмм являться верными. общий количество выделить в тема биграмм также меньше, чем в предыдущий случае, однако значительный они часть являться достаточно точными: из они можно образовать логичный словосочетание латинский письменность, алфавитный письменность, падежный форма, словарный толкование. биграмм ударение_позиция также нельзя назвать случайным: составлять он униграмм часто встречаться в один предложении, например, «… фиксировать ударение ориентироваться на крайний позиция в слово — либо на он начало, либо на конец…» или даже в непосредственный близости: «…особенность фонетика собственно алюторский диалект — противопоставление по долгота в система гласных, …, динамический позиционный ударение…». униграмм число и местоимение, по весь видимости, быть объединить также на основание частый совместный встречаемость в один предложение (несложно представить такой контексты, описывать форма местоимений); однако нельзя утверждать, что они образовать верный биграмму. 3. заключение в настоящий статья быть предложный алгоритм для автоматический выделение биграмм на этап предварительный обработка текст и они последующий добавление в тематический модели. за основа быть взять вероятностный модель латентный размещение дирихле. разработать алгоритм быть проверить на два корпус специальный текстов. достоинство предложить алгоритм заключаться в том, что он позволять выявлять биграмм в корпус текстов, существенно не усложнять модель и не требовать внедрение дополнительный параметров. стоить также отметить, что, благодаря очевидный удобство применение данный метода, он мочь считаться универсальный и применимый для выделение словосочетание в текст разный стиль и тип (как научных, так и художественных, официально-деловой и публицистических). в перспектива планироваться усовершенствовать выделение биграмм, использовать частеречной разметка корпуса. в большинство свой правильно выделить тема формироваться именно из существительное и именной группа [15], поэтому в дальнейший планироваться формировать биграмм в корпус текст преимущественно в соответствие с модель существительное + существительное в родительный падеже, существительное + прилагательное; также не исключить добавление коллокация существительное + глагол, поскольку зачастую такой словосочетание также являться характерный для специальный текстов. наряду с этим, ставиться задача полный исключение формирование такой ошибочный сочетаний, как, например, существительное + наречие или прилагательное + глагол. также планироваться приведение биграмм из лемматизировать форма к согласовать словосочетание путём они повторный поиск в корпус текст и замена на исходный формы. исследование поддержать грант рффи № 16-06-00529 «разработка лингвистический комплекс для автоматический семантический анализ русскоязычный корпус текст с применение статистический методов» (2015– 2018 гг.).